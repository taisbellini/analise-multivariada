---
title: "EST204 - Estatística Multivariada 2020/3 - Prova 2"
author: "Tais Bellini - 205650"
date: "20/01/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Exercício 1

Dados e análise exploratória: 

```{r}
#x1: Comprimento da mandibula
#x2: Largura da mandíula abaixo do primeiro molar
#x3: Largura do condilo articular
#x4: Altura da mandibula abaixo do primeiro molar
#x5: Comprimento do primeiro molar
#x6: Largura do primeiro molar
#x7: Comprimento do primeiro ao terceiro molar
#x8: Comprimento do primeiro ao quarto premolar
#x9: Largura do canino inferior


data = read.table("BD1.txt", header=T,sep="")
colnames(data) = c("id",  "c. mandibula", "l. mand. 1 molar", "l. cond art.", 
                   "a. mand. 1 molar", "c. 1 molar", "l. 1 molar", 
                   "c. 1-3 molar", "c. 1-4 premolar", "l. canino inf.", "raca")
head(data[,2:11])
colMeans(data[,2:10])
```


### Realize a ACP (sem rotação) e apresente os 2 maiores autovalores, os autovetores associados e o percentual acumulado da variação explicada. Interprete a estrutura de correlação dos dados usando os 2 “primeiros” autovetores.

```{r, echo=F, include=F}
library(devtools)
#install_github("vqv/ggbiplot")

library(ggbiplot)
```

Vamos utilizar a função do R _prcomp_ para determinar as componentes principais. Como temos valores discrepantes, vamos utilizar o parâmetro _scale = T_ para padronizar os dados.

```{r}
pca = prcomp(data[,2:10], scale. = T)
paste("Maiores autovalores:")
pca$sdev[1:2]^2
paste("Autovetores associados, respectivamente:")
pca$rotation[,1:2]
```

Observe que obtemos os mesmos resultados através da função _eigen_ do R na matriz de covariância padronizada:

```{r}
eig = eigen(cov(scale(data[,2:10])))

paste("Maiores autovalores funcao eigen:")
eig$values[1:2]
paste("Autovetores associados funcao eigen, respectivamente:")
eig$vectors[,1:2]
```

Agora vamos utilizar a função _summary_ no objeto gerado pela função _prcomp_ para obter a proporção da variância explicada acumulada das duas primeiras componentes principais:
```{r}
summary(pca)
```

Podemos observar que o percentual acumulado da variância explicada das duas primeiras componentes principais é 86%.

```{r}
ggbiplot(pca, main = "Correlação das variáveis com PC1 e PC2") + theme_minimal() 
t(cor(pca$x, data[,2:10])[1:2,])
```

Observa-se que a primeira componente principal é a soma ponderada de todas as variáveis. Já a segunda faz um contraste entre comprimento vs. altura e largura. Ainda, as medidas de dentes individuais possuem menos peso na componente.

### b) Ordene os cães baseados nos escores do primeiro CPs e apresente a raça dos 20 com as maiores medidas. Esse ordenamento faz algum sentido? Justifique!

Vamos observar o número de observações de cada raça e os primeiros 20 animais observados de acordo com a classificação dos escores da primeira componente principal:
```{r}
n = nrow(data)
ss=data.frame(ord=seq(1,n,by=1), y=pca$x[,1], raca = data[,11])
ss_ord = ss[order(ss[,2], decreasing=T),]
summary(data[,11])
ss_ord[1:20,]
```

Oberva-se que a maioria das observações com maior escore na primeira componente principal são da raça Lobos Indianos, sendo que todas as observações desta espécie estão nos primeiros 20, o que indica que é uma raça com predominância destas variáveis observadas, já que a primeira componente principal é uma ponderação de todas as variáveis. Se observarmos abaixo, a média das medidas das observações de Lobos Indianos é superior a média amostral para todas as variáveis.

```{r}
paste("Media das variaveis para Lobos Indianos:")
apply(data[data$raca == "LobosIndianos",2:10], 2, mean)

paste("Media das variaveis de todas as raças:")
apply(data[,2:10], 2, mean)
```

### c) Realize a AF com 2 fatores (utilizando extração das cargas via máxima verossimilhança e rotação varimax) e apresente as cargas fatoriais, as variâncias dos fatores, o percentual acumulado da variação explicada, as comunalidades e a variância não explicada de cada variável. Interprete a estrutura de correlação dos dados usando os 2 primeiros fatores.

Vamos utilizar a função _factanal_ que realiza a análise fatorial através do método de máxima verossimilhança e permite a escolha do método de rotação:
```{r}
fac=factanal(data[2:10],2, rotation="varimax", scores="regression")  
```
  
#### Cargas fatoriais, variância dos fatores e percentual acumulado da variância explicada: 

Abaixo, obtemos os resultados da análise fatorial, onde **Loadings** são as cargas fatoriais dos 2 fatores, **Proportional Var** é a variância de cada fator, (0.44 e 0.37, respectivamente) e **Cumulative Var** o percentual acumulado da variância explicada, que é de 0.82.

```{r}
fac$loadings
```

#### Comunalidades:
Sabemos que a comunalidade é contribuição dos _m_ fatores para explicar a variância da variável _i_. Ela se dá pela seguinte fórmula: 
$$ \sum_{j=1}^{m}{l_{ij}}^2$$ 
Onde: $l_{ij}$ é a j-ésima carga fatorial da variável i. 
Portanto, neste caso em que temos dois fatores, calculamos a comunalidade como a soma dos quadrados das 2 cargas fatoriais para cada variável:

```{r}
comm = apply(fac$loadings, 1, function(l){sum(l^2)})
comm
```

#### Variância específica:
A variância específica, por sua vez, é justamente a parte que não está inclusa nas cargas fatoriais dos fatores comuns, sendo única para aquela variável: 
$$ \psi_i = \sigma_{ii} -  \sum_{j=1}^{m}{l_{ij}}^2$$
Portanto, para os dados em análise, temos:
```{r}
# calculando manualmente, com os dados padronizados
diag(var(scale(data[2:10]))) - comm
```

Padronizamos os dados pois eles possuem valores muito discrepantes entre eles, além de ser o padrão da função _factanal_, usada para esta análise. Podemos obter a variância específica utilizando esta função também, e observamos que os resultados são praticamente os mesmos:

```{r}
v = fac$uniquenesses
v
```

#### Interpretação

```{r}
plot(fac$loadings[,1], fac$loadings[,2], xlab = "Factor 1", ylab = "Factor 2")
text(fac$loadings[,1]+0.04, fac$loadings[,2]-0.008, colnames(data[2:10]), col = "red", ylim= c(-1,1), xlim = c(-1,2))
```

Observamos que, com a rotação, temos as cargas do Fator 1 com uma maior separação entre comprimento e largura/altura do que quando avaliamos via PCA. Neste caso, altura e largura possuem mais peso no Fator 1, enquanto no Fator 2 temos o comprimento com maiores valores.


## Exercicio 2

Dados:

```{r}
remove(list = ls())

data = read.table("nacoes2.txt", header=T,sep="")
colMeans(data[,2:9])
std_data = data
std_data[,2:9] = scale(std_data[,2:9])
```

### a) Proceda a uma Análise de Cluster dos países utilizando dois métodos hierárquicos (o complete linkage e o de Ward) e proponha um agrupamento. Descreva os clusters.

Para ambos os métodos, vamos calcular a distância euclidiana entre os dados e utilizar a função _hclust_ do R selecionando o método desejado. Para avaliar os clusters, vamos buscar o maior "salto" na altura do agrupamento para fazer o corte e definir quantos e quais são os grupos.

#### Complete Linkage

```{r}
d = dist(std_data[,-1], method = "euclidean")
m = as.dist(d, diag = T)

hc_complete=hclust(m, method = "complete") 
h_comp = hc_complete$height
diff_comp = numeric()
for(i in 1:length(h_comp)-1){
  diff_comp[i] = h_comp[i+1] - h_comp[i]
}
paste("Diferença de altura dos agrupamentos:")
diff_comp
```

Observamos um maior salto no último agrupamento, resultando em 2 grupos: 

```{r}
plot(hc_complete, main = "Complete Linkage", hang=-1, labels=data[,1], xlab = "País" )
rect.hclust(hc_complete, k=2, border=1:3)
```

#### Ward

```{r}
hc_ward=hclust(m, method = "ward.D2")
h_ward = hc_ward$height
diff_ward = numeric()
for(i in 1:length(h_ward)-1){
  diff_ward[i] = h_ward[i+1] - h_ward[i]
}
diff_ward
```

Da mesma forma, temos um maior salto no último agrupamento, resultando, também em 2 grupos:

```{r}
plot(hc_ward, main = "Ward", hang=-1, labels=std_data[,1], xlab = "País" )
rect.hclust(hc_ward, k=2, border=1:3)
```

Observa-se que os agrupamentos foram iguais, tendo no primeiro grupo os países Nigéria, Brasil, Arábia Saudita e África do Sul, e no segundo grupo os demais. Vamos analisar as variáveis:
```{r, include=F, echo=F}
require(GGally)
```

Note que, através do método _cutree_, o grupo com Nigéria, Brasil, Arábia Saudita e África do Sul está classificado como 2:
```{r}
grupos=matrix(cutree(hc_complete, h=8), ncol=1)
data = cbind(data, grupos)

colMeans(data[data$grupos == 1,-1])
colMeans(data[data$grupos == 2,-1])
```

Observamos que o grupo 2 (Nigéria, Brasil, Arábia Saudita e África do Sul) é formado por países com maior média de população, apesar de apresentar menor densidade populacional. Ainda, são países com média pior nos indicadores sociais, como expectativa de vida, alfabetização e mortalidade infantil.

### b) Proceda a uma Análise de Cluster dos países utilizando o método k-means [com centroides iniciais aleatórios e semente set.seed(1)]. Defina um o número k de grupos a priori e justifique a escolha de k. Descreva os clusters.

Vamos iniciar o algoritmo k-means com 2 grupos, pois foi o número de grupos obtidos no métodos hierárquicos.

```{r}
remove(list = ls())

data = read.table("nacoes2.txt", header=T,sep="")
colMeans(data[,2:9])
std_data = data
std_data[,2:9] = scale(std_data[,2:9])


set.seed(1)
km=kmeans(std_data[,-1],2) 
fit = cbind(data,km$cluster)

fit[fit$`km$cluster`==1,1]
fit[fit$`km$cluster`==2,1]
```

Observa-se que o agrupamento através do método k-means, com centroides iniciais aleatórios e 2 grupos definidos a priori, obtivemos os mesmos grupos encontrados nos métodos hierárquicos.

Vamos avaliar os grupos através das componentes principais:
```{r}
pc<-prcomp(std_data[,-1], scale.= T)
z<-pc$x

plot(z[,1],z[,2],pch=16,col=km$cluster,xlab="pc1", ylab="pc2", 
     main="Classificação por k-means em 2 grupos")
text(z,labels=data[,1])

```

Observamos que, de fato, há uma separação entre estes dois grupos quando olhamos para a primeira componente principal.

## Exercício 3

Dados:

$$ X_1 \sim N_3(\mu_1, \Sigma), X_2 \sim N_3(\mu_2, \Sigma)$$


$$ \mu_1 = \begin{bmatrix} -3 \\ 1 \\ 4 \end{bmatrix}$$

$$ \mu_2 = \begin{bmatrix} -1 \\ 0 \\ 3 \end{bmatrix}$$
$$\Sigma = \begin{bmatrix}
            1 & -2 & 0 \\
            -2 & 5 & 0 \\
            0 & 0 & 2
            \end{bmatrix}$$
            
Ou seja,

$$ f_1 \sim N_3(\mu_1, \Sigma), f_2 \sim N_3(\mu_2, \Sigma)$$
            
            
### a) Admitindo os custos de classificação errada c(2/1) = 100,00 e c(1/2) = 50,00 e probabilidades a priori p1 = 0.7 e p2 = 0.3, encontre e mostre os coeficientes do vetor discriminante e a constante discriminante.

Como temos que as populações seguem uma distribuição normal ($f_1$ e $f_2$), podemos determinar que, dada uma observação $x_0$, classificamos ela na populacão $\pi_1$ se:

$$h(\mu_1, \mu_2, \Sigma) \geq ln\left(\frac{c(1|2)p_2}{c(2|1)p1}\right)$$
No qual: 

$$h(\mu_1, \mu_2, \Sigma) = (\mu_1-\mu_2)^\intercal\Sigma^{-1}x_0-\frac{1}{2}(\mu_1-\mu_2)^\intercal\Sigma^{-1}(\mu_1+\mu_2)$$

Definindo $a = (\mu_1-\mu_2)^\intercal\Sigma^{-1}$, $m = \frac{1}{2}a^\intercal(\mu_1+\mu_2)$ e $y=a^\intercal x_0$, temos que os coeficientes do vetor determinante se dão pelo vetor $a^\intercal$:

$$ a = (\mu_1-\mu_2)^\intercal\Sigma^{-1} = \left[-2, 1, 1\right] \begin{bmatrix}
            5 & 2 & 0 \\
            2 & 1 & 0 \\
            0 & 0 & 0.5
            \end{bmatrix} = \left[-8, 3, 0.5\right]$$ 

Assim, separando os elementos constantes dos variáveis da equação, temos que, dado uma observação $x_0$, $$ y = \left[-8, 3, 0.5\right]\begin{bmatrix} x_{01} \\ x_{02} \\x_{03} \end{bmatrix}$$

Se este valor de y for maior ou igual à constante discriminante definida abaixo, classificamos a observação no grupo $\pi_1$, caso contrário, no grupo $\pi_2$. 

$$ m + ln\left(\frac{c(1|2)p_2}{c(2|1)p1}\right) = 16.25 + ln\left(\frac{15}{70}\right) = 16.25 - 1.54 = 14.71$$

Calculando no R:
```{r}
mu1 = matrix(c(-3, 1, 4), ncol = 1)
mu2 = matrix(c(-1, 0, 3), ncol = 1)
sigma = cbind(c(1, -2, 0), c(-2, 5, 0), c(0, 0, 2))
c12 = 50
p2 = 0.3
c21 = 100
p1 = 0.7

at = t((mu1-mu2))%*%solve(sigma)
paste("Vetor discriminante a':")
at
paste("Constante discriminante:")
m = (1/2)%*%at%*%(mu1+mu2)
m + log((c12*p2)/(c21*p1))
```


### b) Considerando c(2/1) = c(1/2) e p1 = p2, calcule o TOE dessa regra. Classifique uma nova observação x' =[0,1,4]. 

Se considerarmos $c(2|1) = c(1|2)$ e $p_1 = p_2$, então teremos que a razão $\frac{c(1|2)p_2}{c(2|1)p1}$ será igual a 1, e, portanto, seu logaritmo natural igual a zero. Assim, basta verificarmos se $$y = a^\intercal x_0 \geq m = 16.25$$ para definir se classificaremos uma nova observação no grupo $\pi_1$ ou no grupo $\pi_2$. 

A taxa ótima de erro (TOE), neste caso, é obtida pela própria probabilidade total de classificação incorreta (PTCI) se o calcularmos através de um mecanismo de classificação ótimo. 

Para calcular o PTCI, queremos definir as probabilidades de erro:

$$PTCI = P(E_1) + P(E_2)$$

$$ P(E_2) = P(2|1)p_1 = p_1P(Y_1 < m)$$ 
$$ P(E_1) = P(1|2)p_2 = p_2P(Y_2 \geq m)$$

Onde $Y_i \sim N_1(a^\intercal\mu_i, \Delta^2), \Delta^2 = a^\intercal(\mu_1-\mu_2), i = 1,2$

Normalizando para a variável $Z$, temos:

$$ P(Y_1 < m) = P(Y<\frac{1}{2}a^\intercal(\mu_1+\mu_2)) = P\left(Z < \left(\frac{1}{2}a^\intercal(\mu_1+\mu_2) - a^\intercal\mu_1)\right)\frac{1}{\Delta}\right) = P(Z < -\frac{\Delta}{2}) = \phi(-\frac{\Delta}{2})$$

Analogamente, $P(Y_2 \geq m) = \phi(-\frac{\Delta}{2})$ 

Portanto, 

$$TOE = PTCI = P(E_1) + P(E_2) = p_1\phi(-\frac{\Delta}{2}) + p_2\phi(-\frac{\Delta}{2}) = (p_1 + p_2)\phi(-\frac{\Delta}{2})$$

Como $p_1 = p_2$, temos que $TOE = \phi(-\frac{\Delta}{2}) = \phi(-\frac{\sqrt{a^\intercal(\mu_1-\mu_2)}}{2})$.

Calculando no R:
```{r}
mu1 = matrix(c(-3, 1, 4), ncol = 1)
mu2 = matrix(c(-1, 0, 3), ncol = 1)

at = t((mu1-mu2))%*%solve(sigma)

delta = sqrt(at%*%(mu1-mu2))
paste("TOE:")
pnorm(-(delta/2))

```


### c) Usando a semente set.seed(1), simule uma amostra de tamanho 100 de cada uma das populações, ajuste a função de Ficher aos dados (usando função lda do R), obtenha a matriz de confusão e o TAE. Mostre os coeficientes do vetor discriminante. 

```{r, echo=F, include=F}
require(MASS)
require(mvtnorm)
```

Simulacão dos dados:
```{r}
n = 100
mu1 = c(-3, 1, 4)
mu2 = c(-1, 0, 3)
sigma = cbind(c(1, -2, 0), c(-2, 5, 0), c(0, 0, 2))
set.seed(1)
x1 = rmvnorm(n, mean = mu1, sigma = sigma)
x2 = rmvnorm(n, mean = mu2, sigma = sigma)

data = rbind(x1, x2)
data = cbind(data, c(rep(1, 100), rep(2,100)))

colnames(data) = c("x1", "x2", "x3", "pop")
data = as.data.frame(data)
```

Utilizando a função _lda_ do R para obter os resultados da análise discriminante nos dados gerados:
```{r}
ad = lda(data[,4]~., data[,1:3], prior = c(0.3, 0.7))
ad
```

Para obter a matriz de confusão, vamos classificar as observações geradas baseado nos coeficientes encontrados pela função e determinar quantas classificações foram feitas corretamente e quantas incorretamente para cada grupo:

```{r}
pred = predict(ad)$class

conf_matrix = table(data[,4],pred)
conf_matrix
```

A taxa aparente de erro (TAE) se dá pela razão entre a soma das classificações erradas e o número total de observações:

```{r}
TAE=(conf_matrix[1,2]+conf_matrix[2,1])/nrow(data)
TAE
```

A TAE encontrada foi de `r TAE`.
