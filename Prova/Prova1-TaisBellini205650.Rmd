---
title: "EST204 - Estatística Multivariada 2020/3 - Prova 1"
author: "Tais Bellini - 205650"
date: "06/12/2020"
output: pdf_document
---

## Exercício 1

Dados:

$$ X = \begin{bmatrix}
        y_1 & y_2 \\
        1 & 1 \\
        2 & 2 \\
        3 & 4 \\
        4 & 9 \\ 
        \end{bmatrix}$$

### a) Obtenha os vetores coluna de diferenças. A partir deles, obtenha a matriz S de covariâncias e a matriz R de correlações amostrais.

Para calcular as distâncias, primeiros determinamos o vetor de médias $\mathbf{\bar{x}}^\intercal = [\bar{x_1}, \bar{x_2}]$: 
$$ \mathbf{\bar{x}} = \begin{bmatrix} 
                      \frac{1+2+3+4}{4} \\
                      \frac{1+2+4+9}{4}
                      \end{bmatrix} = 
                      \begin{bmatrix} 
                      \frac{5}{2} \\
                      4
                      \end{bmatrix}$$
Agora, calculamos a diferença entre cada coluna $\mathbf{y_i}$ e $\mathbf{\bar{x}}$, obtendo os vetores de diferenças:

$$\mathbf{d_1} = \mathbf{y_1} - \bar{x_1}\mathbf{1} = \begin{bmatrix} 1 - 2.5 \\ 2 - 2.5 \\ 3-2.5 \\ 4-2.5 \end{bmatrix} = \begin{bmatrix} -1.5 \\ -0.5 \\ 0.5 \\ 1.5 \end{bmatrix} $$
$$\mathbf{d_2} = \mathbf{y_2} - \bar{x_2}\mathbf{1} = \begin{bmatrix} 1 - 4 \\ 2 - 4 \\ 4-4 \\ 9-4 \end{bmatrix} = \begin{bmatrix} -3 \\ -2 \\ 0 \\ 5 \end{bmatrix} $$
A partir dos vetores $\mathbf{d_1}$ e $\mathbf{d_2}$, calculamos o seu comprimento ao quadrado: $$L_{d_1}^2 = \mathbf{d_1}^\intercal\mathbf{d_1} = (-1.5)^2 + (-0.5)^2 + 0.5^2 + 1.5^2 = 5$$ $$L_{d_2}^2 = \mathbf{d_2}^\intercal\mathbf{d_2} = (-3)^2 + (-2)^2 + 0^2 + 5^2 = 38$$ $$L_{d_1,d_2}^2 = \mathbf{d_1}^\intercal\mathbf{d_2} = (-1.5)(-3) + (-0.5)(-2) + (0.5)(0) + (1.5)(5) = 13$$

Note que os comprimentos acima nada mais são que a soma dos quadrados da diferença entre cada observação e a média amostral: $$L_{d_i}^2 = \sum_{j=1}^{n}{(x_{j,i}-\bar{x})^2}$$ Ou seja, é a variância $S_{ii}$ sem a divisão por $(n-1)$. Portanto, temos que:
$$L_{d_i}^2 = \mathbf{d_i}^\intercal\mathbf{d_i} = (n-1)S_{ii}$$ $$L_{d_1,d_2}^2 = \mathbf{d_1}^\intercal\mathbf{d_2} = (n-1)S_{12}$$

Portanto, temos que: $$L_{d_1}^2 = 5 = (4-1)S_{11} \\ S_{11} = \frac{5}{3}$$ $$L_{d_2}^2 = 38 = (4-1)S_{22} \\ S_{22} = \frac{38}{3}$$ $$L_{d_1,d_2}^2 = 13 = (4-1)S_{12} \\ S_{12} = \frac{13}{3}$$

Logo, a matriz de covariâncias $S$ se dá por: 
$$ S = \begin{bmatrix}
    \frac{5}{3} & \frac{13}{3} \\
    \frac{13}{3} & \frac{38}{3}
    \end{bmatrix} $$
    
A partir dos valores da matriz de covariâncias, obtemos facilmente o coeficiente de correlação $r_{12} = \frac{S12}{\sqrt{S11}\sqrt{S22}} = \frac{\frac{13}{3}}{\sqrt{\frac{5}{3}}\sqrt{\frac{38}{3}}} = 0.943$. Logo, a matrix de correlações amostrais $R$ se dá por: 
$$ R = \begin{bmatrix}
    1 & 0.943 \\
    0.943 & 1
    \end{bmatrix} $$

Conferindo os resultados no R: 

```{r}
X = cbind(c(1,2,3,4), c(1,2,4,9))
x_barra = colMeans(X)
x_barra
S = cov(X)
S
R = cor(X)
R
```


### b) Calcule a medida de variância generalizada a partir de S e R e mostre a relação entre elas.

A medida de variância generalizada a partir de S é o determinante da matriz de covariância:
$$ S = \begin{bmatrix}
    \frac{5}{3} & \frac{13}{3} \\
    \frac{13}{3} & \frac{38}{3}
    \end{bmatrix}$$
    
$$|S| = \left(\frac{5}{3}\right)\left(\frac{38}{3}\right) - \left(\frac{13}{3}\right)\left(\frac{13}{3}\right) = 2.33 $$

    
Esta medida é proporcional ao quadrado do volume gerado pelos vetores de distância. Por isso, ela pode ser bastante afetada pela variância de uma determinada variável, pois se $S_{ii}$ é muito grande, teremos $d_i$ muito grande e, por consequência, um volume muito grande. 

Para evitar estas distorções, podemos padronizar os vetores de distância, substituindo cada elemento da amostra pelo seu valor padronizado: $x_{ij} = \frac{(x_{ij} - \bar{x_j})}{\sqrt{S_{jj}}}$. A matriz S deste conjunto de dados padronizado é a matriz de correlação R. Portanto, temos que a medida de variância generalizada padronizada é o determinante da matriz R: 

$$ R = \begin{bmatrix}
    1 & 0.943 \\
    0.943 & 1
    \end{bmatrix} $$ 
    
$$ |R| =  (1)(1) - (0.943)(0.943) = 0.11 $$

Observe que |S| e |R| estão relacionados da seguinte forma: 
  $$ |S| = (S_{11}..S_{pp})|R| $$

Podemos entender esta relação a partir da definição da matriz de correlação amostral R: 
    
$$ R = \begin{bmatrix}
\frac{1}{\sqrt{S_{11}}} &  & \\
&  \ddots & \\
& & \frac{1}{\sqrt{S_{pp}}}
\end{bmatrix} S \begin{bmatrix}
\frac{1}{\sqrt{S_{11}}} &  & \\
&  \ddots & \\
& & \frac{1}{\sqrt{S_{pp}}}
\end{bmatrix} $$
      
Sabendo que o determinante de uma multiplicação é o produto do determinante de cada fator $|XY| = |X||Y|$ e que o determinante de uma matriz diagonal é o produto dos elementos da diagonal, temos que: 
$$ |R| = \left(\frac{1}{\sqrt{S_{11}}...\sqrt{S_{pp}}}\right)|S|\left(\frac{1}{\sqrt{S_{11}}...\sqrt{S_{pp}}}\right) \\
|S| = (S_{11}...S_{pp})|R|$$
          
No caso dos nossos dados, temos que:
$$ |S| = 2.33 = \left(\frac{5}{3}\right)\left(\frac{38}{3}\right)(0.11) $$
            
Validando os resultados no R: 
            
```{r}
VG = det(S)
VG

VG_p = det(R)
VG_p
```
          
          
### c) Calcule a variância total utilizando a matriz S. Comente as diferenças entre as duas medidas.

A medida de variância total é a soma dos elementos da diagonal principal da matriz S: 
  $$ S_{t} = S_{11}+S_{22} = \left(\frac{5}{3}\right) + \left(\frac{38}{3}\right) = 14.33 $$
  
A variância total é a soma do quadrado distâncias $\mathbf{d_1} ... \mathbf{d_{j}}$, ou seja, do comprimento destes vetores, dividida por $(n-1)$, sem levar em consideração a orientação dos vetores. Já variância generalizada considera as covariâncias, que determinam o ângulo entre os vetores e influenciam no volume final.

## Exercicio 2

Dados: 
  
$$ X \sim N_3(\mathbf{\mu}, \Sigma)$$ 
$$\Sigma = \begin{bmatrix}
1 & -2 & 0 \\ 
-2 & 5 & 0 \\
0 & 0 & 2
\end{bmatrix}$$
$$\mathbf{\mu}^\intercal = [-3, 1, 4] $$ 

### a) Escreva genericamente a forma quadrática $d^2 = (\mathbf{x} - x\mathbf{\mu})^\intercal\Sigma^{-1}(\mathbf{x} - \mathbf{\mu})$ decomposta em função de uma soma envolvendo autovetores e autovalores de $\Sigma$.

$d^2$ pode ser escrito como: 
$$ d^2 = \sum_{i=1}^{p}{\frac{1}{\lambda_i}[(\mathbf{x}-\mathbf{\mu})^\intercal\mathbf{e_i}]^2}$$
onde $[\lambda_i, \mathbf{e_i}]$ são os autopares de $\Sigma$ (ou seja, autovalores e autovetores da matriz de covariância $\Sigma$).

### b) Encontre o comprimento dos eixos do elipsoide formado por $d^2$, com 0.95 de probabilidade (Encontre os autovalores no R usando a função _eigen_).

Primeiro, vamos determinar os autovalores e autovetores: 

```{r}
sigma = cbind(c(1,-2,0), c(-2,5,0), c(0,0,2))
mu = as.matrix(c(-3, 1, 4))

eig = eigen(sigma)
eig
```

Para definir o contorno da elipsoide de 95% de confiança, vamos utilizar o fato de que $(x-\mathbf{\mu})^\intercal\Sigma^{-1}(x-\mathbf{\mu}) \sim \chi_p^2$. Queremos uma elipse com área que contemple 95% dos nossos dados que possuem distribuição Normal de três variáveis, portanto, 
$(x-\mathbf{\mu})^\intercal\Sigma^{-1}(x-\mathbf{\mu}) = c^2$ onde $c^2$ é o quantil da distribuição Qui-Quadrado tal que $P(\chi_3^2 \leq c^2) = 0.95$.

Encontrando $c^2$ e calculando o comprimento dos eixos partindo do centro ($\mathbf{\mu}$):
```{r}
c2 = qchisq(0.95, 3)
c2

eixo1 = sqrt(c2)*sqrt(eig$values[1])
eixo1
eixo2 = sqrt(c2)*sqrt(eig$values[2])
eixo2
eixo3 = sqrt(c2)*sqrt(eig$values[3])
eixo3
```


Assim, temos que o comprimento dos eixos a partir do centro ($\mathbf{\mu}$) se dão por: 
$$ eixo_1 = c\sqrt{\lambda_1}\mathbf{e_1} = \sqrt{`r c2`}\sqrt{`r eig$values[1]`}[`r eig$vectors[,1]`] = `r eixo1`$$
$$ eixo_2 = c\sqrt{\lambda_2}\mathbf{e_2} = \sqrt{`r c2`}\sqrt{`r eig$values[2]`}[`r eig$vectors[,2]`] = `r eixo2`$$
$$ eixo_3 = c\sqrt{\lambda_3}\mathbf{e_3} = \sqrt{`r c2`}\sqrt{`r eig$values[3]`}[`r eig$vectors[,3]`] = `r eixo3`$$

A orientação se dá pelos vetores: $$e_1^\intercal = [`r eig$vectors[,1]`] $$ $$ e_2^\intercal = [`r eig$vectors[,2]`]$$ $$ e_3^\intercal = [`r eig$vectors[,3]`]$$

### c) Encontre a distribuição do vetor aleatório $\mathbf{Y}^\intercal = (Y_1, Y_2)$, onde $Y_1 = \frac{(X_1+X_2)}{2}$ e $Y_2 = 2X_1-X_2+X_3$

Sabemos que, se $X\sim N(\mathbf{\mu}, \Sigma)$,  combinações lineares dos componentes de X possuem distribuição normal e que os subsets de componentes de X também são normalmente distribuídos. 
Podemos reescrever $\mathbf{Y}$ como uma combinação $AX$:

$$\mathbf{Y} = \begin{bmatrix}
                Y_1 \\ Y_2 
                \end{bmatrix} = \begin{bmatrix}
                                \frac{(X_1+X_2)}{2} \\ 
                                2X_1-X_2+X_3
                                \end{bmatrix}$$
                                
$$ A = \begin{bmatrix}
        \frac{1}{2} & \frac{1}{2} & 0 \\
        2 & -1 & 1 
        \end{bmatrix}$$

$$ AX = \begin{bmatrix}
        \frac{1}{2}X_1 + \frac{1}{2}X_2 \\
        2X_1 - 1X_2 + 1X_3 
        \end{bmatrix}$$

Utilizando o *Resultado 4.3* do livro, temos que $AX \sim N_2(A\mathbf{\mu}, A\Sigma A^\intercal)$, onde:
$$ A\mathbf{\mu} = \begin{bmatrix}
                    \frac{1}{2} & \frac{1}{2} & 0 \\
                    2 & -1 & 1 
                    \end{bmatrix} \begin{bmatrix} -3 \\ 1 \\ 4 \end{bmatrix} = \begin{bmatrix} -1 \\ -3 \end{bmatrix}$$

$$ A \Sigma A^\intercal = \begin{bmatrix}
                    \frac{1}{2} & \frac{1}{2} & 0 \\
                    2 & -1 & 1
                    \end{bmatrix} \begin{bmatrix}
                                    1 & -2 & 0 \\ 
                                    -2 & 5 & 0 \\
                                    0 & 0 & 2
                                    \end{bmatrix} \begin{bmatrix} 
                                                  \frac{1}{2} & 2 \\
                                                  \frac{1}{2} & -1 \\
                                                            0 & 1 
                                                  \end{bmatrix} = 
\begin{bmatrix}
\frac{1}{2} & - \frac{5}{2} \\
- \frac{5}{2} & 19 \end{bmatrix} $$

Conferindo os resultados no R: 

```{r}
A = rbind(c(1/2, 1/2, 0), c(2, -1, 1))
sigma = cbind(c(1,-2, 0), c(-2, 5, 0), c(0,0,2))
t(A)
A%*%sigma%*%t(A)
mu = as.matrix(c(-3,1,4))
A%*%mu
```


## Exercício 3

Dados: 

```{r, include = F, echo = F}
require(GGally)
require(mvShapiroTest)
require(car)
require(carData)
require(ggplot2)
require(gtools)
```


```{r}
#x1: education (Gasto per capita com educação)
#x2: income (Renda per capita)
#x3: young (Proporção da pop abaixo de 18 anos)
#x4: urban (Proporção d pop na área urbana)
x=carData::Anscombe
head(x)
```


### a) Aplique o teste de Shapiro de normalidade univariada e bivariada. Informe o valor p de cada teste e conclua a respeito.

O teste Shapiro de normalidade se baseia na estatística $W$ e testa a hipótese nula de que os dados da amostra provém de uma distribuição normal. 
Dada um amostra $y_1, ..., y_n$, a estatística $W$ é dada por: 
$$ W = \frac{b^2}{s^2} $$
onde, 
$$ s^2 = \sum_{i=1}^{n}(y_i - \bar{y})^2 $$
Se $n$ é par, $n = 2k$ e $b$ é calculado por:
$$ b = \sum_{i=1}^{k}a_{n-i+1}(y{n-i+1} - y_i)$$
Se $n$ é ímpar, $n=2k+1$ e $b$ se calcula da seguinte forma:
$$ b = a_n(y_n-y_1) + ... + a_{k+2}(y_{k+2} - y_k) $$
Os valores de $a$ são tabelados e dependem de $n$. 

#### Univariado

Para executar o teste, podemos utilizar uma função pronta do R e analisar o p-valor resultante. 

No caso do teste univariado, vamos executar o teste Shapiro para cada variável separadamente e avaliar a sua normalidade através dos p-valores encontrados:

```{r}
W=rep(0,ncol(x))

for(i in 1:4){
  W[i]=shapiro.test(x[,i])$p.value
}

W = t(as.matrix(W))
colnames(W) = c("x1", "x2", "x3", "x4")
rownames(W) = c("p-value")
W
```

Observa-se pelos resultados que não rejeitamos a hipótese nula de que as variáveis $x_2$ e $x_4$ provém de uma distribuição normal, se avaliados nos níveis de significância $\alpha = 0.01$ $\alpha = 0.05$ e $\alpha = 0.10$. Já para a variável $x1$, rejeitamos para a avaliacão de $\alpha = 0.05$ e $\alpha = 0.10$ e não rejeitamos para $\alpha = 0.01$. Por fim, não há evidência de que os dados coletados da variável $x_3$ venham de uma distribuição normal.

Vamos analisar graficamente os resultados acima: 

```{r}
par(mfrow=c(2,2))
for( i in 1:ncol(x)){
  qqPlot(x[,i], dist="norm", main=paste("x",i), ylab=paste("empirical"))
}
```

Observa-se que, de fato, $x_3$ possui diversos pontos fora da banda de confiança no _Q-Q Plot_ e $x_1$, apesar de mais próximo de uma reta do que $x_3$, também possui ponto fora da banda.


#### Bivariado

Vamos primeiro observar graficamente as variáveis duas a duas através do _scatter plot_:

```{r}
data = as.data.frame(x)
ggpairs(data)
```

Observamos que os pares $(x_1, x_2)$ e $(x_2,x_4)$ são os que mais se aproximam de uma elipse, indicando normalidade. Os outros pares, no entanto, também apresentam formato similar a uma elipse. Vamos verificar abaixo realizando o teste Shapiro multivariado.

Assim como no caso anterior, utilizamos uma função do R, mas desta vez voltada para o teste multivariado:

```{r}
pares = combinations(4, 2, 1:4, repeats.allowed = F)
x=as.matrix(x)
W_biv = rep(0,nrow(pares))
for (i in 1:nrow(pares)) {
  vars = pares[i,]
  W_biv[i] = mvShapiro.Test(x[,vars])$p.value
}

resultados = cbind(pares, W_biv)
colnames(resultados) = c("V1", "V2", "p-value")
rownames(resultados) = c("(x1,x2)","(x1,x3)", "(x1,x4)", "(x2,x3)", "(x2,x4)", "(x3,x4)")
resultados
```

Observa-se, pelos p-valores encontrados, que, de fato, há evidência de que o par $(x_2, x_4)$ seja proveniente de uma distribuição normal se avaliados com os níveis de significância $\alpha = 0.1$, $\alpha = 0.5$ e $\alpha = 0.01$. Já para o par $(x_1, x_2)$, também podemos aceitar a hipótese nula de que venha de uma distribuição normal, com significância $\alpha = 0.05$ e $\alpha = 0.05$. Não rejeitamos a hipótese de normalidade para o par $(x_1, x_3)$ com nível de significância $\alpha = 0.01$ e rejeitamos a hipótese para os demais casos.

Podemos ainda observar o caso multivariado: 

```{r}
mvShapiro.Test(x)
```

Concluímos que, no caso multivariado, aceitamos a hipótese de normalidade com os níveis de significância $\alpha = 0.1$, $\alpha = 0.5$ e $\alpha = 0.01$.

### b) Teste a hipótese a 5% de que o vetor de média populacional seja $\mu^\intercal = [210, 3225, 360, 665]$. Informe o valor da estatística de teste, o valor crítico e conclua a respeito.

Sabemos que $T^2 = n(\mathbf{\bar{x}} - \mathbf{\mu_0})^\intercal S^{-1} (\mathbf{\bar{x}} - \mathbf{\mu_0})$. Ainda, $(\mathbf{\bar{x}} - \mathbf{\mu_0})^\intercal S^{-1} (\mathbf{\bar{x}} - \mathbf{\mu_0})$ é a distância estatítica entre $\bar{x}$ e $\mu_0$. Assim, usamos a função _mahalanobis_ do R para calcular $T^2$:


```{r}
x_barra = colMeans(x)
S=cov(x)
n = nrow(x)
p = ncol(x)
alpha = 0.05
mu = c(210, 3225, 360, 665)

T2_cal<-n*mahalanobis(x_barra, mu, S, inverted = FALSE)
T2_cal
```

Temos, então, que $T^2$ = `r T2_cal`.

Rejeitamos $H_0$ se $$ T^2 > \frac{(n-1)p}{n-p}F_{p,n-p}(\alpha) $$ onde $$ T^2 = n*(\mathbf{\bar{x}} - \mathbf{\mu})^\intercal S^{-1} (\mathbf{\bar{x}} - \mathbf{\mu}) $$. No caso dos nossos dados, o valor crítico se dá por: $$ q = \frac{(51-1)4}{51-4}F_{4,51-4}(\alpha = 0.05) = \frac{200}{47}F_{4,47}(0.05)$$.

```{r}
q=qf(1-alpha,p,n-p)*((n-1)*p)/(n-p)
q
```
Temos, então, que $q$ = `r q`.

Como $T^2$ = `r T2_cal` > q = `r q`, concluímos por rejeitar $H_0: \mathbf{\mu} = [210, 3225, 360, 665]$ com nível de significância de 5%.

### c) Para cada variável construa os intervalos T e bonferroni e conclua a respeito.

Os intervalos de confiança simultâneos seguem a seguinte lógica:

Dado **a** um vetor não aleatório qualquer e $X_1, X_2, ..., X_n$ uma amostra aleatória de $X \sim N_p(\mathbf{\mu}, \mathbf{\Sigma})$, sabemos que $\mathbf{a^\intercal X} \sim N_p(\mathbf{a^\intercal\mu}, \mathbf{a^\intercal\Sigma a})$. Assim pode-se verificar que $\left[\mathbf{a^\intercal\bar{X}} \pm \sqrt{\frac{p(n-1)}{(n-p)}F_{p,n-p}}\mathbf{a^\intercal}S\mathbf{a}\right]$ contém $\mathbf{a^\intercal\mu}$ com probabilidade $1-\alpha$. Portanto, para determinar a i-ésima média, escolhemos **a** como um vetor de zeros com o valor 1 na i-ésima posição. Assim, temos que: $$IC_{T^2}(\mu_i, 1-\alpha) = \left[\bar{x_i} \pm \sqrt{\frac{p(n-1)}{(n-p)}F_{p,n-p,\alpha}\frac{S_i^2}{n}}\right]$$

Como testamos no exercício 3-a, podemos assumir que os dados são provenientes de uma distribuição normal, portanto, podemos utilizar os resultados acima.  

Calculando o $IC_{T^2}$ para $\mathbf{\mu_1, \mu_2, \mu_3}$ e $\mathbf{\mu_4}$:

```{r}
# Calculando os intervalos para mu1, mu2 e mu3
Ls=c()
Li=c()
for (i in 1:p) {
  Ls[i]=(x_barra[i])+sqrt(q*S[i,i]/n)
  Li[i]=(x_barra[i])-sqrt(q*S[i,i]/n)
}

Lim=rbind(Ls,Li)
colnames(Lim)<-c("x1","x2", "x3", "x4")
Lim
```

Estes resultados, porém, garantem $1-\alpha$ de confiança apenas no caso univariado, ou seja, neste caso, temos 95% de confiança que $\mu_1$ está dentro do intervalo `r Lim[,1]`, $\mu_2$ está em [`r Lim[,2]`], [`r Lim[,3]`] contém $\mu_3$ e [`r Lim[,4]`] contempla $\mu_4$. Porém, não temos o mesmo nível de confiança de que um ponto $(\mu_1, \mu_2, \mu_3, \mu_4)$ esteja nos intervalos determinados. Para se obter uma confiança global de $1-\alpha$ utiliza-se a correção de Bonferroni: 
$$ IC(\mu_i, 1-\alpha) = \left[\bar{x_i} \pm t_{n-1, \frac{\alpha}{2p}}\sqrt{\frac{S_i^2}{n}}\right]$$

```{r}
Lsb=c()
Lib=c()
for (i in 1:p) {
  Lsb[i]=(x_barra[i])+qt(1-(alpha/(2*p)),n-1)*sqrt(S[i,i]/n)
  Lib[i]=(x_barra[i])-qt(1-(alpha/(2*p)),n-1)*sqrt(S[i,i]/n)
}

Limb=rbind(Lsb,Lib)
colnames(Limb)<-c("x1", "x2", "x3", "x4")
Limb
```

Observa-se que os limites diminuem o seu volume total, sendo os limites superiores do IC sem correção maiores para todas as variáveis e os limites inferiores menores. Nas duas medidas de intervalo de confiança, nota-se rejeitamos $H_0$ pois  $\mu_1 = 210$ está fora dos intervalos. 

## Exercício 4 - Um programa de reforço de estudos foi avaliado a partir de uma amstra de 15 estudantes. Cada um deles realizou provas com conteúdos de matemática, física e química antes e depois de serem submetidos ao programa de reforço.

### a) Aplique o teste de Shapiro de normalidade multivariada. Informe o valor p de cada teste e conclua a respeito.

Dados: 

```{r}
mat_a=c(6.62, 5.53, 6.27, 5.49, 5.26, 6.46, 2.64, 3.89, 6.23, 4.36,
        6.08, 5.70, 6.91, 4.70, 5.14)
fis_a=c(6.08, 6.82, 5.68, 4.80, 6.62, 5.86, 7.26, 4.49, 6.44, 6.32,
        6.28, 7.07, 5.58, 5.76, 6.22)
qui_a=c(5.43, 6.39, 5.41, 6.10, 4.71, 5.22, 5.63, 7.27, 5.92, 3.34,
        6.96, 7.77, 4.37, 4.80, 6.33)

mat_d=c(8.00, 6.87, 6.12, 5.49, 5.74, 5.94, 4.87, 7.92, 6.79, 9.59,
        5.49, 8.34, 8.29, 6.77, 7.66)
fis_d=c(6.18, 7.82, 4.68, 4.00, 3.62, 5.86, 8.26, 4.30, 6.44, 8.32,
        6.28, 7.07, 5.58, 6.76, 6.00)
qui_d=c(8.06, 8.43, 6.39, 7.14, 6.78, 6.83, 7.93, 7.31, 9.11, 7.62,
        7.72, 8.32, 9.12, 7.18, 8.98)

x=cbind(mat_a,fis_a,qui_a,mat_d,fis_d,qui_d)


d_mat=mat_d-mat_a
d_fis=fis_d-fis_a
d_qui=qui_d-qui_a
d<-cbind(d_mat,d_fis,d_qui)

head(x)
```


Testando normalidade multivariada através do teste Shapiro multivariado:

```{r}
shap = mvShapiro.Test(x)
shap
```

Observamos que o p-valor é bem alto: `r  shap$p.value`, portanto, aceitamos a hipótese $H_0$: de que  $\mathbf{X_1, ...,X_n}$ provém de uma população com distribuição normal. 

Vamos avaliar caso multivariado separado por matéria:
```{r}
mat = cbind(mat_a, mat_d)
fis = cbind(fis_a, fis_d)
qui = cbind(qui_a, qui_d)

shap_mat = mvShapiro.Test(mat)
shap_fis = mvShapiro.Test(fis)
shap_qui = mvShapiro.Test(qui)

p_vals = t(as.matrix(c(shap_mat$p.value, shap_fis$p.value, shap_qui$p.value)))
colnames(p_vals) = c("mat", "fis", "qui")
rownames(W) = c("p-value")
p_vals

```

Observamos que, novamente, os p-valores para cada par de variáveis referentes a cada matéria são altos, confirmando a hipótese de normalidade. 

Vamos avaliar o caso univariado: 

```{r}
W=rep(0,ncol(x))

for(i in 1:ncol(x)){
  W[i]=shapiro.test(x[,i])$p.value
}

W = t(as.matrix(W))
colnames(W) = c("mat_a", "fis_a", "qui_a", "mat_d", "fis_d", "qui_d")
rownames(W) = c("p-value")
W
```

Por fim, temos evidência de que cada variável provém de uma distribuição normal.

Vamos ajustar o banco de forma que a matéria seja um fator e possamos observar os dados: 

```{r}
mat_f = cbind(mat_a, mat_d, rep(1, length(mat_a)))
fis_f = cbind(fis_a, fis_d, rep(2, length(fis_a)))
qui_f = cbind(qui_a, qui_d, rep(3, length(qui_a)))

x_f = rbind(mat_f, fis_f, qui_f)
x_f = as.data.frame(x_f)
colnames(x_f) = c("Antes", "Depois", "Materia")

ggpairs(x_f[,-3], mapping=aes(colour=as.factor(x_f[,3])), upper=list(combo= 'blank', continuous='blank'), title = "Gráfico 4.1") 

```

No gráfico acima, temos vermelho para a matéria Matemática, verde para Física e azul para Química.

### b) O programa de reforço teve influência nas notas? Teste com 1% de significância Informe o valor da estatística de teste, o valor crítico e conclua a respeito.

Para determinar se o programa de reforço teve influência nas notas, precisamos testar $H_0 : \mathbf{\mu_a} = \mathbf{\mu_d}$. Ou seja, partimos da hipótese nula de a média das notas antes é igual a média depois do reforço e, portanto, ele não teve influência nas notas. Outra forma de escrever $H_0: \mathbf{\mu_a} - \mathbf{\mu_d} = 0$

Como verificamos que há evidências de que os dados são de uma população normal e desconhecemos $\Sigma$, vamos calcular a distância entre as notas antes e depois do reforço para cada matéria e aplicar o teste com a estatística $T^2$:
```{r}
d_barra = colMeans(d)
S = cov(d)
mu=c(0,0,0)
n=nrow(d)
p=ncol(d)
alpha = 0.01

T2_cal<-n*mahalanobis(d_barra, mu, S, inverted = FALSE)
T2_cal

q=((n-1)*p)/(n-p)*qf(1-alpha,p,n-p)
q
```
Como $T^2$ = `r T2_cal` > q = `r q`, concluímos por rejeitar $H_0: \mathbf{\mu_a} - \mathbf{\mu_d} = 0$ com nível de significância de 1%, indicando que há evidência de que o reforço influenciou as notas dos alunos. 

### c) Construa os intervalos T e bonferroni e conclua a respeito.

o cálculo para os intervalos é muito similar ao da questão 3. Neste caso, em que estamos avaliando a diferença entre médias, utilizamos a diferença ($\mathbf{\bar{x_a}} - \mathbf{\bar{x_d}}$) no lugar de $\mathbf{\bar{x}}$:


Intervalo T: 
```{r}
Ls=c()
Li=c()
for (i in 1:p) {
  Ls[i]=(d_barra[i])+sqrt(q*S[i,i]/n)
  Li[i]=(d_barra[i])-sqrt(q*S[i,i]/n)
  
}

Lim=rbind(Ls,Li)
colnames(Lim) = c("Mat", "Fis", "Qui")
Lim
```

Bonferroni:
```{r} 
Lsb=c()
Lib=c()
for (i in 1:p) {
  Lsb[i]=(d_barra[i])+qt(1-(alpha/(2*p)),n-1)*sqrt(S[i,i]/n)
  Lib[i]=(d_barra[i])-qt(1-(alpha/(2*p)),n-1)*sqrt(S[i,i]/n)
  
}
Limb=rbind(Lsb,Lib)
colnames(Limb) = c("Mat", "Fis", "Qui")
Limb
```

Observa-se que, em ambos os intervalos, apenas a matéria de Química não engloba o zero, ou seja, o reforço só foi influente nesta matéria. Isto também pode ser observado no Gráfico 4.1, onde observamos que a média dos dados da matéria de Química se moveu mais significativamente em relação as outras.  

## Exercício 5

Dados: 

```{r}
x=mtcars

#Fator = forma do motor
#vs=0 V-shaped
#vs=1 Straight

# quereos as variaveis mpg, hp, wt, qsec
X=x[,c(1,4,6,7,8)]
head(X)
```


### a) Aplique o teste de Shapiro de normalidade multivariada dentro de cada grupo (motores em V e convencionais). Informe o valor p de cada teste e conclua a respeito. Em caso de não normalidade, verifique se os dados transformados (via logaritmo natural) aderem a hipótese de normalidade. Neste caso realizar as análises subsequentes utilizando os dados transformados.

Separando os dados por fator:
```{r}
v_shaped = X[X$vs == 0,]
straight = X[X$vs == 1,]
```

Testando normalidade multivariada através do teste Shapiro multivariado:
```{r}
v_shaped = as.matrix(v_shaped)
straight = as.matrix(straight)
shap_v = mvShapiro.Test(v_shaped[,-5])
shap_s = mvShapiro.Test(straight[,-5])

resultados = t(as.matrix(c(shap_v$p.value, shap_s$p.value)))
colnames(resultados) = c("V-shaped", "Straight")
rownames(resultados) = c("p-value")
resultados
```

Não rejeitamos a hipótese de normalidade para o grupo _V-shaped_, mas rejeitamos para o grupo _Straight_.

Transformando os dados do grupo _Straight_:

```{r}
straight_log = apply(straight[,-5], 2, log)
straight_log = cbind(straight_log, rep(nrow(straight_log), 1))

shap_slog = mvShapiro.Test(straight_log[,-5])
shap_slog
```

Observa-se que, com a transformação realizada no grupo _Straight_, passamos a aceitar a hipótese de normalidade avaliando com nível de significância $\alpha = 0.1$, $\alpha = 0.5$ e $\alpha = 0.01$.

Para manter a consistência dos dados, vamos aplicar o log no grupo _V-shaped_ também:

```{r}
v_shaped_log = apply(v_shaped[,-5], 2, log)
v_shaped_log = cbind(v_shaped_log, rep(nrow(v_shaped_log), 0))

shap_vlog = mvShapiro.Test(v_shaped_log[,-5])
shap_vlog
```

Com a transformação, o grupo _V_shaped_ também adere à hipoótese de normalidade. Portanto, para os próximos cálculos, utilizaremos os dados transformados. 

Analisando no caso univariado: 

```{r}
W=rep(0,ncol(v_shaped_log[,-5]))

for(i in 1:ncol(v_shaped_log[,-5])){
  W[i]=shapiro.test(v_shaped_log[,i])$p.value
}

W = t(as.matrix(W))
colnames(W) = c("mpg", "hp", "wt", "qsec")
rownames(W) = c("p-value: v-shaped")

Ws=rep(0,ncol(straight[,-5]))

for(i in 1:ncol(straight[,-5])){
  Ws[i]=shapiro.test(straight[,i])$p.value
}

Ws = t(as.matrix(Ws))
colnames(Ws) = c("mpg", "hp", "wt", "qsec")
rownames(Ws) = c("p-value: straight")

p = rbind(W, Ws)
p

```

Com nível de significância $\alpha = 0.01$, aceitamos a hipótese de normalidade para todas as variáveis transformadas.


#### b) Existe a 5% diferença significativa nas variáveis analisadas entre os 2 tipos de motores? Informe o valor da estatística de teste, o valor cr ́ıtico e conclua a respeito.

