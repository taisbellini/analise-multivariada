---
title: "EST204 - Estatística Multivariada 2020/3 - Prova 1"
author: "Tais Bellini - 205650"
date: "06/12/2020"
output: pdf_document
---

## Exercício 1

Dados:

$$ X = \begin{bmatrix}
        y_1 & y_2 \\
        1 & 1 \\
        2 & 2 \\
        3 & 4 \\
        4 & 9 \\ 
        \end{bmatrix}$$

### a) Obtenha os vetores coluna de diferenças. A partir deles, obtenha a matriz S de covariâncias e a matriz R de correlações amostrais.

Para calcular as distâncias, primeiros determinamos o vetor de médias $\mathbf{\bar{x}}^\intercal = [\bar{x_1}, \bar{x_2}]$: 
$$ \mathbf{\bar{x}} = \begin{bmatrix} 
                      \frac{1+2+3+4}{4} \\
                      \frac{1+2+4+9}{4}
                      \end{bmatrix} = 
                      \begin{bmatrix} 
                      \frac{5}{2} \\
                      4
                      \end{bmatrix}$$
Agora, calculamos a diferença entre cada coluna $\mathbf{y_i}$ e $\mathbf{\bar{x}}$, obtendo os vetores de diferenças:

$$\mathbf{d_1} = \mathbf{y_1} - \bar{x_1}\mathbf{1} = \begin{bmatrix} 1 - 2.5 \\ 2 - 2.5 \\ 3-2.5 \\ 4-2.5 \end{bmatrix} = \begin{bmatrix} -1.5 \\ -0.5 \\ 0.5 \\ 1.5 \end{bmatrix} $$
$$\mathbf{d_2} = \mathbf{y_2} - \bar{x_2}\mathbf{1} = \begin{bmatrix} 1 - 4 \\ 2 - 4 \\ 4-4 \\ 9-4 \end{bmatrix} = \begin{bmatrix} -3 \\ -2 \\ 0 \\ 5 \end{bmatrix} $$
A partir dos vetores $\mathbf{d_1}$ e $\mathbf{d_2}$, calculamos o seu comprimento ao quadrado: $$L_{d_1}^2 = \mathbf{d_1}^\intercal\mathbf{d_1} = (-1.5)^2 + (-0.5)^2 + 0.5^2 + 1.5^2 = 5$$ $$L_{d_2}^2 = \mathbf{d_2}^\intercal\mathbf{d_2} = (-3)^2 + (-2)^2 + 0^2 + 5^2 = 38$$ $$L_{d_1,d_2}^2 = \mathbf{d_1}^\intercal\mathbf{d_2} = (-1.5)(-3) + (-0.5)(-2) + (0.5)(0) + (1.5)(5) = 13$$

Note que os comprimentos acima nada mais são que a soma dos quadrados da diferença entre cada observação e a média amostral: $$L_{d_i}^2 = \sum_{j=1}^{n}{(x_{j,i}-\bar{x})^2}$$ Ou seja, é a variância $S_{ii}$ sem a divisão por $(n-1)$. Portanto, temos que:
$$L_{d_i}^2 = \mathbf{d_i}^\intercal\mathbf{d_i} = (n-1)S_{ii}$$ $$L_{d_1,d_2}^2 = \mathbf{d_1}^\intercal\mathbf{d_2} = (n-1)S_{12}$$

Portanto, temos que: $$L_{d_1}^2 = 5 = (4-1)S_{11} \\ S_{11} = \frac{5}{3}$$ $$L_{d_2}^2 = 38 = (4-1)S_{22} \\ S_{22} = \frac{38}{3}$$ $$L_{d_1,d_2}^2 = 13 = (4-1)S_{12} \\ S_{12} = \frac{13}{3}$$

Logo, a matriz de covariâncias $S$ se dá por: 
$$ S = \begin{bmatrix}
    \frac{5}{3} & \frac{13}{3} \\
    \frac{13}{3} & \frac{38}{3}
    \end{bmatrix} $$
    
A partir dos valores da matriz de covariâncias, obtemos facilmente o coeficiente de correlação $r_{12} = \frac{S12}{\sqrt{S11}\sqrt{S22}} = \frac{\frac{13}{3}}{\sqrt{\frac{5}{3}}\sqrt{\frac{38}{3}}} = 0.943$. Logo, a matrix de correlações amostrais $R$ se dá por: 
$$ R = \begin{bmatrix}
    1 & 0.943 \\
    0.943 & 1
    \end{bmatrix} $$

Conferindo os resultados no R: 

```{r}
X = cbind(c(1,2,3,4), c(1,2,4,9))
x_barra = colMeans(X)
x_barra
S = cov(X)
S
R = cor(X)
R
```


### b) Calcule a medida de variância generalizada a partir de S e R e mostre a relação entre elas.

A medida de variância generalizada a partir de S é o determinante da matriz de covariância:
$$ S = \begin{bmatrix}
    \frac{5}{3} & \frac{13}{3} \\
    \frac{13}{3} & \frac{38}{3}
    \end{bmatrix}$$
    
$$|S| = \left(\frac{5}{3}\right)\left(\frac{38}{3}\right) - \left(\frac{13}{3}\right)\left(\frac{13}{3}\right) = 2.33 $$

    
Esta medida é proporcional ao quadrado do volume gerado pelos vetores de distância. Por isso, ela pode ser bastante afetada pela variância de uma determinada variável, pois se $S_{ii}$ é muito grande, teremos $d_i$ muito grande e, por consequência, um volume muito grande. 

Para evitar estas distorções, podemos padronizar os vetores de distância, substituindo cada elemento da amostra pelo seu valor padronizado: $x_{ij} = \frac{(x_{ij} - \bar{x_j})}{\sqrt{S_{jj}}}$. A matriz S deste conjunto de dados padronizado é a matriz de correlação R. Portanto, temos que a medida de variância generalizada padronizada é o determinante da matriz R: 

$$ R = \begin{bmatrix}
    1 & 0.943 \\
    0.943 & 1
    \end{bmatrix} $$ 
    
$$ |R| =  (1)(1) - (0.943)(0.943) = 0.11 $$

Observe que |S| e |R| estão relacionados da seguinte forma: 
  $$ |S| = (S_{11}..S_{pp})|R| $$

Podemos entender esta relação a partir da definição da matriz de correlação amostral R: 
    
$$ R = \begin{bmatrix}
\frac{1}{\sqrt{S_{11}}} &  & \\
&  \ddots & \\
& & \frac{1}{\sqrt{S_{pp}}}
\end{bmatrix} S \begin{bmatrix}
\frac{1}{\sqrt{S_{11}}} &  & \\
&  \ddots & \\
& & \frac{1}{\sqrt{S_{pp}}}
\end{bmatrix} $$
      
Sabendo que o determinante de uma multiplicação é o produto do determinante de cada fator $|XY| = |X||Y|$ e que o determinante de uma matriz diagonal é o produto dos elementos da diagonal, temos que: 
$$ |R| = \left(\frac{1}{\sqrt{S_{11}}...\sqrt{S_{pp}}}\right)|S|\left(\frac{1}{\sqrt{S_{11}}...\sqrt{S_{pp}}}\right) \\
|S| = (S_{11}...S_{pp})|R|$$
          
No caso dos nossos dados, temos que:
$$ |S| = 2.33 = \left(\frac{5}{3}\right)\left(\frac{38}{3}\right)(0.11) $$
            
Validando os resultados no R: 
            
```{r}
VG = det(S)
VG

VG_p = det(R)
VG_p
```
          
          
### c) Calcule a variância total utilizando a matriz S. Comente as diferenças entre as duas medidas.

A medida de variância total é a soma dos elementos da diagonal principal da matriz S: 
  $$ S_{t} = S_{11}+S_{22} = \left(\frac{5}{3}\right) + \left(\frac{38}{3}\right) = 14.33 $$
  
A variância total é a soma do quadrado distâncias $\mathbf{d_1} ... \mathbf{d_{j}}$, ou seja, do comprimento destes vetores, dividida por $(n-1)$, sem levar em consideração a orientação dos vetores. Já variância generalizada considera as covariâncias, que determinam o ângulo entre os vetores e influenciam no volume final.

## Exercicio 2

Dados: 
  
$$ X \sim N_3(\mathbf{\mu}, \Sigma) \\ 
\Sigma = \begin{bmatrix}
1 & -2 & 0 \\ 
-2 & 5 & 0 \\
0 & 0 & 2
\end{bmatrix} \\ \mathbf{\mu}^\intercal = [-3, 1, 4] $$ 

### a) Escreva genericamente a forma quadrática $d^2 = (\mathbf{x} - x\mathbf{\mu})^\intercal\Sigma^{-1}(\mathbf{x} - \mathbf{\mu})$ decomposta em função de uma soma envolvendo autovetores e autovalores de $\Sigma$.

$d^2$ pode ser escrito como: 
$$ d^2 = \sum_{i=1}^{p}{\frac{1}{\lambda_i}[(\mathbf{x}-\mathbf{\mu})^\intercal\mathbf{e_i}]^2}$$
onde $[\lambda_i, \mathbf{e_i}]$ são os autopares de $\Sigma$ (ou seja, autovalores e autovetores da matriz de covariância $\Sigma$).

### b) Encontre o comprimento dos eixos do elipsoide formado por $d^2$, com 0.95 de probabilidade (Encontre os autovalores no R usando a função _eigen_).

Primeiro, vamos determinar os autovalores e autovetores: 

```{r}
sigma = cbind(c(1,-2,0), c(-2,5,0), c(0,0,2))
mu = as.matrix(c(-3, 1, 4))

eig = eigen(sigma)
eig
```

Para definir o contorno da elipsoide de 95% de confiança, vamos utilizar o fato de que $(x-\mathbf{\mu})^\intercal\Sigma^{-1}(x-\mathbf{\mu}) \sim \chi_p^2$. Queremos uma elipse com área que contemple 95% dos nossos dados que possuem distribuição Normal de três variáveis, portanto, 
$(x-\mathbf{\mu})^\intercal\Sigma^{-1}(x-\mathbf{\mu}) = c^2$ onde $c^2$ é o quantil da distribuição Qui-Quadrado tal que $P(\chi_3^2 \leq c^2) = 0.95$.

Encontrando $c^2$ e calculando o comprimento dos eixos partindo do centro ($\mathbf{\mu}$):
```{r}
c2 = qchisq(0.95, 3)
c2

eixo1 = sqrt(c2)*sqrt(eig$values[1])
eixo1
eixo2 = sqrt(c2)*sqrt(eig$values[2])
eixo2
eixo3 = sqrt(c2)*sqrt(eig$values[3])
eixo3
```


Assim, temos que o comprimento dos eixos a partir do centro ($\mathbf{\mu}$) se dão por: 
$$ eixo_1 = c\sqrt{\lambda_1}\mathbf{e_1} = \sqrt{`r c2`}\sqrt{`r eig$values[1]`}[`r eig$vectors[,1]`] = `r eixo1`$$
$$ eixo_2 = c\sqrt{\lambda_2}\mathbf{e_2} = \sqrt{`r c2`}\sqrt{`r eig$values[2]`}[`r eig$vectors[,2]`] = `r eixo2`$$
$$ eixo_3 = c\sqrt{\lambda_3}\mathbf{e_3} = \sqrt{`r c2`}\sqrt{`r eig$values[3]`}[`r eig$vectors[,3]`] = `r eixo3`$$

A orientação se dá pelos vetores: $$e_1^\intercal = [`r eig$vectors[,1]`] $$ $$ e_2^\intercal = [`r eig$vectors[,2]`]$$ $$ e_3^\intercal = [`r eig$vectors[,3]`]$$

### c) Encontre a distribuição do vetor aleatório $\mathbf{Y}^\intercal = (Y_1, Y_2)$, onde $Y_1 = \frac{(X_1+X_2)}{2}$ e $Y_2 = 2X_1-X_2+X_3$

Sabemos que, se $X\sim N(\mathbf{\mu}, \Sigma)$,  combinações lineares dos componentes de X possuem distribuição normal e que os subsets de componentes de X também são normalmente distribuídos. 
Podemos reescrever $\mathbf{Y}$ como uma combinação $AX$:

$$\mathbf{Y} = \begin{bmatrix}
                Y_1 \\ Y_2 
                \end{bmatrix} = \begin{bmatrix}
                                \frac{(X_1+X_2)}{2} \\ 
                                2X_1-X_2+X_3
                                \end{bmatrix}$$
                                
$$ A = \begin{bmatrix}
        \frac{1}{2} & \frac{1}{2} & 0 \\
        2 & -1 & 1 
        \end{bmatrix}$$

$$ AX = \begin{bmatrix}
        \frac{1}{2}X_1 + \frac{1}{2}X_2 \\
        2X_1 - 1X_2 + 1X_3 
        \end{bmatrix}$$

Utilizando o *Resultado 4.3* do livro, temos que $AX \sim N_2(A\mathbf{\mu}, A\Sigma A^\intercal)$, onde:
$$ A\mathbf{\mu} = \begin{bmatrix}
                    \frac{1}{2} & \frac{1}{2} & 0 \\
                    2 & -1 & 1 
                    \end{bmatrix} \begin{bmatrix} -3 \\ 1 \\ 4 \end{bmatrix} = \begin{bmatrix} -1 \\ -3 \end{bmatrix}$$

$$ A \Sigma A^\intercal = \begin{bmatrix}
                    \frac{1}{2} & \frac{1}{2} & 0 \\
                    2 & -1 & 1
                    \end{bmatrix} \begin{bmatrix}
                                    1 & -2 & 0 \\ 
                                    -2 & 5 & 0 \\
                                    0 & 0 & 2
                                    \end{bmatrix} \begin{bmatrix} 
                                                  \frac{1}{2} & 2 \\
                                                  \frac{1}{2} & -1 \\
                                                            0 & 1 
                                                  \end{bmatrix} = 
\begin{bmatrix}
\frac{1}{2} & - \frac{5}{2} \\
- \frac{5}{2} & 19 \end{bmatrix} $$
                    
```{r}
A = rbind(c(1/2, 1/2, 0), c(2, -1, 1))
sigma = cbind(c(1,-2, 0), c(-2, 5, 0), c(0,0,2))
t(A)
A%*%sigma%*%t(A)
mu = as.matrix(c(-3,1,4))
A%*%mu
```


## Exercício 3

Dados: 

```{r, include = F, echo = F}
require(GGally)
require(mvShapiroTest)
require(car)
require(carData)
require(ggplot2)
require(gtools)
```


```{r}
#x1: education (Gasto per capita com educação)
#x2: income (Renda per capita)
#x3: young (Proporção da pop abaixo de 18 anos)
#x4: urban (Proporção d pop na área urbana)
x=carData::Anscombe
head(x)
```


### a) Aplique o teste de Shapiro de normalidade univariada e bivariada. Informe o valor p de cada teste e conclua a respeito.

O teste Shapiro de normalidade se baseia na estatística $W$ e testa a hipótese nula de que os dados da amostra provém de uma distribuição normal. 
Dada um amostra $y_1, ..., y_n$, a estatística $W$ é dada por: 
$$ W = \frac{b^2}{s^2} $$
onde, 
$$ s^2 = \sum_{i=1}^{n}(y_i - \bar{y})^2 $$
Se $n$ é par, $n = 2k$ e $b$ é calculado por:
$$ b = \sum_{i=1}^{k}a_{n-i+1}(y{n-i+1} - y_i)$$
Se $n$ é ímpar, $n=2k+1$ e $b$ se calcula da seguinte forma:
$$ b = a_n(y_n-y_1) + ... + a_{k+2}(y_{k+2} - y_k) $$
Os valores de $a$ são tabelados e dependem de $n$. 

#### Univariado

Para executar o teste, podemos utilizar uma função pronta do R e analisar o p-valor resultante. 

No caso do teste univariado, vamos executar o teste Shapiro para cada variável separadamente e avaliar a sua normalidade através dos p-valores encontrados:

```{r}
W=rep(0,ncol(x))

for(i in 1:4){
  W[i]=shapiro.test(x[,i])$p.value
}

W = t(as.matrix(W))
colnames(W) = c("x1", "x2", "x3", "x4")
rownames(W) = c("p-value")
W
```

Observa-se pelos resultados que não rejeitamos a hipótese nula de que as variáveis $x_2$ e $x_4$ provém de uma distribuição normal, se avaliados nos níveis de significância $\alpha = 0.01$ $\alpha = 0.05$ e $\alpha = 0.10$. Já para a variável $x1$, rejeitamos para a avaliacão de $\alpha = 0.05$ e $\alpha = 0.10$ e não rejeitamos para $\alpha = 0.01$. Por fim, não há evidência de que os dados coletados da variável $x_3$ venham de uma distribuição normal.

Vamos analisar graficamente os resultados acima: 

```{r}
par(mfrow=c(2,2))
for( i in 1:ncol(x)){
  qqPlot(x[,i], dist="norm", main=paste("x",i), ylab=paste("empirical"))
}
```

Observa-se que, de fato, $x_3$ possui diversos pontos fora da banda de confiança no _Q-Q Plot_ e $x_1$, apesar de mais próximo de uma reta do que $x_3$, também possui ponto fora da banda.


#### Bivariado

Vamos primeiro observar graficamente as variáveis duas a duas através do _scatter plot_:

```{r}
data = as.data.frame(x)
ggpairs(data)
```

Observamos que os pares $(x_1, x_2)$ e $(x_2,x_4)$ são os que mais se aproximam de uma elipse, indicando normalidade. Os outros pares, no entanto, também apresentam formato similar a uma elipse. Vamos verificar abaixo realizando o teste Shapiro multivariado.

Assim como no caso anterior, utilizamos uma função do R, mas desta vez voltada para o teste multivariado:

```{r}
pares = combinations(4, 2, 1:4, repeats.allowed = F)
x=as.matrix(x)
W_biv = rep(0,nrow(pares))
for (i in 1:nrow(pares)) {
  vars = pares[i,]
  W_biv[i] = mvShapiro.Test(x[,vars])$p.value
}

resultados = cbind(pares, W_biv)
colnames(resultados) = c("V1", "V2", "p-value")
rownames(resultados) = c("(x1,x2)","(x1,x3)", "(x1,x4)", "(x2,x3)", "(x2,x4)", "(x3,x4)")
resultados
```

Observa-se, pelos p-valores encontrados, que, de fato, há evidência de que o par $(x_2, x_4)$ seja proveniente de uma distribuição normal se avaliados com os níveis de significância $\alpha = 0.1$, $\alpha = 0.5$ e $\alpha = 0.01$. Já para o par $(x_1, x_2)$ também podemos aceitar a hipótese nula de que venha de uma distribuição normal, com significância $\alpha = 0.05$ e $\alpha = 0.05$. Não rejeitamos a hipótese de normalidade para o par $(x_1, x_3)$ com nível de significância $\alpha = 0.01$ e rejeitamos a hipótese para os demais casos.

Podemos ainda observar o caso multivariado: 

```{r}
mvShapiro.Test(x)
```

Concluímos que, no caso multivariado, aceitamos a hipótese de normalidade com os níveis de significância $\alpha = 0.1$, $\alpha = 0.5$ e $\alpha = 0.01$.

### b) Teste a hipótese a 5% de que o vetor de média populacional seja $\mu^\intercal = [210, 3225, 360, 665]$. Informe o valor da estatística de teste, o valor crítico e conclua a respeito.

Sabemos que $T^2 = n(\mathbf{\bar{x}} - \mathbf{\mu_0})^\intercal S^{-1} (\mathbf{\bar{x}} - \mathbf{\mu_0})$. Ainda, $(\mathbf{\bar{x}} - \mathbf{\mu_0})^\intercal S^{-1} (\mathbf{\bar{x}} - \mathbf{\mu_0})$ é a distância estatítica entre $\bar{x}$ e $\mu_0$. Assim, usamos a função _mahalanobis_ do R para calcular $T^2$:


```{r}
x_barra = colMeans(x)
S=cov(x)
n = nrow(x)
p = ncol(x)
n
p
mu = c(210, 3225, 360, 665)

T2_cal<-n*mahalanobis(x_barra, mu, S, inverted = FALSE)
T2_cal
```

Temos, então, que $T^2$ = `r T2_cal`.

Rejeitamos $H_0$ se $$ T^2 > \frac{(n-1)p}{n-p}F_{p,n-p}(\alpha) $$ onde $$ T^2 = n*(\mathbf{\bar{x}} - \mathbf{\mu})^\intercal S^{-1} (\mathbf{\bar{x}} - \mathbf{\mu}) $$. No caso dos nossos dados, o valor crítico se dá por: $$ q = \frac{(51-1)4}{51-4}F_{4,51-4}(\alpha = 0.05) = \frac{200}{47}F_{4,47}(0.05)$$.

```{r}
q=qf(1-alpha,p,n-p)*((n-1)*p)/(n-p)
q
```
Temos, então, que $q$ = `r q`.

Como $T^2$ = `r T2_cal` > q = `r q`, concluímos por rejeitar $H_0: \mathbf{\mu} = [210, 3225, 360, 665]$ com nível de significância de 5%.

### c) Para cada variável construa os intervalos T e bonferroni e conclua a respeito.

Os intervalos de confiança simultâneos seguem a seguinte lógica:

Dado **a** um vetor não aleatório qualquer e $X_1, X_2, ..., X_n$ uma amostra aleatória de $X \sim N_p(\mathbf{\mu}, \mathbf{\Sigma})$, sabemos que $\mathbf{a^\intercal X} \sim N_p(\mathbf{a^\intercal\mu}, \mathbf{a^\intercal\Sigma a})$. Assim pode-se verificar que $\left[\mathbf{a^\intercal\bar{X}} \pm \sqrt{\frac{p(n-1)}{(n-p)}F_{p,n-p}}\mathbf{a^\intercal}S\mathbf{a}\right]$ contém $\mathbf{a^\intercal\mu}$ com probabilidade $1-\alpha$. Portanto, para determinar a i-ésima média, escolhemos **a** como um vetor de zeros com o valor 1 na i-ésima posição. Assim, temos que: $$IC_{T^2}(\mu_i, 1-\alpha) = \left[\bar{x_i} \pm \sqrt{\frac{p(n-1)}{(n-p)}F_{p,n-p,\alpha}\frac{S_i^2}{n}}\right]$$

Como testamos no exercício 3-a, podemos assumir que os dados são provenientes de uma distribuição normal, portanto, podemos utilizar os resultados acima.  

Calculando o $IC_{T^2}$ para $\mathbf{\mu_1, \mu_2, \mu_3}$ e $\mathbf{\mu_4}$:

```{r}
# Calculando os intervalos para mu1, mu2 e mu3
Ls=c()
Li=c()
for (i in 1:p) {
  Ls[i]=(x_barra[i])+sqrt(q*S[i,i]/n)
  Li[i]=(x_barra[i])-sqrt(q*S[i,i]/n)
}

Lim=rbind(Ls,Li)
colnames(Lim)<-c("x1","x2", "x3", "x4")
Lim
```

Estes resultados, porém, garantem $1-\alpha$ de confiança apenas no caso univariado, ou seja, neste caso, temos 95% de confiança que $\mu_1$ está dentro do intervalo `r Lim[,1]`, $\mu_2$ está em `r Lim[,2]`, `r Lim[,3]` contém $\mu_3$ e `r Lim[,4]` contempla $\mu_4$. Porém, não temos o mesmo nível de confiança de que um ponto $(\mu_1, \mu_2, \mu_3, \mu_4)$ esteja nos intervalos determinados. Para se obter uma confiança global de $1-\alpha$ utiliza-se a correção de Bonferroni: 
$$ IC(\mu_i, 1-\alpha) = \left[\bar{x_i} \pm t_{n-1, \frac{\alpha}{2p}}\sqrt{\frac{S_i^2}{n}}\right]$$

```{r}
Lsb=c()
Lib=c()
for (i in 1:p) {
  Lsb[i]=(x_barra[i])+qt(1-(alpha/(2*p)),n-1)*sqrt(S[i,i]/n)
  Lib[i]=(x_barra[i])-qt(1-(alpha/(2*p)),n-1)*sqrt(S[i,i]/n)
}

Limb=rbind(Lsb,Lib)
colnames(Limb)<-c("x1", "x2", "x3", "x4")
Limb
```

Observa-se que os limites diminuem o seu volume total, sendo os limites superiores do IC sem correção maiores para todas as variáveis e os limites inferiores menores. Nas duas medidas de intervalo de confiança, nota-se rejeitamos $H_0$ pois  $\mu_1 = 210$ está fora dos intervalos. 


