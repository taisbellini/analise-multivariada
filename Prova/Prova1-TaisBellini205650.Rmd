---
title: "EST204 - Estatística Multivariada 2020/3 - Prova 1"
author: "Tais Bellini - 205650"
date: "06/12/2020"
output: pdf_document
---

## Exercício 1

Dados:

$$ X = \begin{bmatrix}
        y_1 & y_2 \\
        1 & 1 \\
        2 & 2 \\
        3 & 4 \\
        4 & 9 \\ 
        \end{bmatrix}$$

### a) Obtenha os vetores coluna de diferenças. A partir deles, obtenha a matriz S de covariâncias e a matriz R de correlações amostrais.

Para calcular as distâncias, primeiros determinamos o vetor de médias $\mathbf{\bar{x}}^\intercal = [\bar{x_1}, \bar{x_2}]$: 
$$ \mathbf{\bar{x}} = \begin{bmatrix} 
                      \frac{1+2+3+4}{4} \\
                      \frac{1+2+4+9}{4}
                      \end{bmatrix} = 
                      \begin{bmatrix} 
                      \frac{5}{2} \\
                      4
                      \end{bmatrix}$$
Agora, calculamos a diferença entre cada coluna $\mathbf{y_i}$ e $\mathbf{\bar{x}}$, obtendo os vetores de diferenças:

$$\mathbf{d_1} = \mathbf{y_1} - \bar{x_1}\mathbf{1} = \begin{bmatrix} 1 - 2.5 \\ 2 - 2.5 \\ 3-2.5 \\ 4-2.5 \end{bmatrix} = \begin{bmatrix} -1.5 \\ -0.5 \\ 0.5 \\ 1.5 \end{bmatrix} $$
$$\mathbf{d_2} = \mathbf{y_2} - \bar{x_2}\mathbf{1} = \begin{bmatrix} 1 - 4 \\ 2 - 4 \\ 4-4 \\ 9-4 \end{bmatrix} = \begin{bmatrix} -3 \\ -2 \\ 0 \\ 5 \end{bmatrix} $$
A partir dos vetores $\mathbf{d_1}$ e $\mathbf{d_2}$, calculamos o seu comprimento ao quadrado: $$L_{d_1}^2 = \mathbf{d_1}^\intercal\mathbf{d_1} = (-1.5)^2 + (-0.5)^2 + 0.5^2 + 1.5^2 = 5$$ $$L_{d_2}^2 = \mathbf{d_2}^\intercal\mathbf{d_2} = (-3)^2 + (-2)^2 + 0^2 + 5^2 = 38$$ $$L_{d_1,d_2}^2 = \mathbf{d_1}^\intercal\mathbf{d_2} = (-1.5)(-3) + (-0.5)(-2) + (0.5)(0) + (1.5)(5) = 13$$

Note que os comprimentos acima nada mais são que a soma dos quadrados da diferença entre cada observação e a média amostral: $$L_{d_i}^2 = \sum_{j=1}^{n}{(x_{j,i}-\bar{x})^2}$$ Ou seja, é a variância $S_{ii}$ sem a divisão por $(n-1)$. Portanto, temos que:
$$L_{d_i}^2 = \mathbf{d_i}^\intercal\mathbf{d_i} = (n-1)S_{ii}$$ $$L_{d_1,d_2}^2 = \mathbf{d_1}^\intercal\mathbf{d_2} = (n-1)S_{12}$$

Portanto, temos que: $$L_{d_1}^2 = 5 = (4-1)S_{11} \\ S_{11} = \frac{5}{3}$$ $$L_{d_2}^2 = 38 = (4-1)S_{22} \\ S_{22} = \frac{38}{3}$$ $$L_{d_1,d_2}^2 = 13 = (4-1)S_{12} \\ S_{12} = \frac{13}{3}$$

Logo, a matriz de covariâncias $S$ se dá por: 
$$ S = \begin{bmatrix}
    \frac{5}{3} & \frac{13}{3} \\
    \frac{13}{3} & \frac{38}{3}
    \end{bmatrix} $$
    
A partir dos valores da matriz de covariâncias, obtemos facilmente o coeficiente de correlação $r_{12} = \frac{S12}{\sqrt{S11}\sqrt{S22}} = \frac{\frac{13}{3}}{\sqrt{\frac{5}{3}}\sqrt{\frac{38}{3}}} = 0.943$. Logo, a matrix de correlações amostrais $R$ se dá por: 
$$ R = \begin{bmatrix}
    1 & 0.943 \\
    0.943 & 1
    \end{bmatrix} $$

Conferindo os resultados no R: 

```{r}
X = cbind(c(1,2,3,4), c(1,2,4,9))
x_barra = colMeans(X)
x_barra
S = cov(X)
S
R = cor(X)
R
```


### b) Calcule a medida de variância generalizada a partir de S e R e mostre a relação entre elas.

A medida de variância generalizada a partir de S é o determinante da matriz de covariância:
$$ S = \begin{bmatrix}
    \frac{5}{3} & \frac{13}{3} \\
    \frac{13}{3} & \frac{38}{3}
    \end{bmatrix}$$
    
$$|S| = \left(\frac{5}{3}\right)\left(\frac{38}{3}\right) - \left(\frac{13}{3}\right)\left(\frac{13}{3}\right) = 2.33 $$

    
Esta medida é proporcional ao quadrado do volume gerado pelos vetores de distância. Por isso, ela pode ser bastante afetada pela variância de uma determinada variável, pois se $S_{ii}$ é muito grande, teremos $d_i$ muito grande e, por consequência, um volume muito grande. 

Para evitar estas distorções, podemos padronizar os vetores de distância, substituindo cada elemento da amostra pelo seu valor padronizado: $x_{ij} = \frac{(x_{ij} - \bar{x_j})}{\sqrt{S_{jj}}}$. A matriz S deste conjunto de dados padronizado é a matriz de correlação R. Portanto, temos que a medida de variância generalizada padronizada é o determinante da matriz R: 

$$ R = \begin{bmatrix}
    1 & 0.943 \\
    0.943 & 1
    \end{bmatrix} $$ 
    
$$ |R| =  (1)(1) - (0.943)(0.943) = 0.11 $$

Observe que |S| e |R| estão relacionados da seguinte forma: 
  $$ |S| = (S_{11}..S_{pp})|R| $$

Podemos entender esta relação a partir da definição da matriz de correlação amostral R: 
    
$$ R = \begin{bmatrix}
\frac{1}{\sqrt{S_{11}}} &  & \\
&  \ddots & \\
& & \frac{1}{\sqrt{S_{pp}}}
\end{bmatrix} S \begin{bmatrix}
\frac{1}{\sqrt{S_{11}}} &  & \\
&  \ddots & \\
& & \frac{1}{\sqrt{S_{pp}}}
\end{bmatrix} $$
      
Sabendo que o determinante de uma multiplicação é o produto do determinante de cada fator $|XY| = |X||Y|$ e que o determinante de uma matriz diagonal é o produto dos elementos da diagonal, temos que: 
$$ |R| = \left(\frac{1}{\sqrt{S_{11}}...\sqrt{S_{pp}}}\right)|S|\left(\frac{1}{\sqrt{S_{11}}...\sqrt{S_{pp}}}\right) \\
|S| = (S_{11}...S_{pp})|R|$$
          
No caso dos nossos dados, temos que:
$$ |S| = 2.33 = \left(\frac{5}{3}\right)\left(\frac{38}{3}\right)(0.11) $$
            
Validando os resultados no R: 
            
```{r}
VG = det(S)
VG

VG_p = det(R)
VG_p
```
          
          
### c) Calcule a variância total utilizando a matriz S. Comente as diferenças entre as duas medidas.

A medida de variância total é a soma dos elementos da diagonal principal da matriz S: 
  $$ S_{t} = S_{11}+S_{22} = \left(\frac{5}{3}\right) + \left(\frac{38}{3}\right) = 14.33 $$
  
A variância total é a soma do quadrado distâncias $\mathbf{d_1} ... \mathbf{d_{j}}$, ou seja, do comprimento destes vetores, dividida por $(n-1)$, sem levar em consideração a orientação dos vetores. Já variância generalizada considera as covariâncias, que determinam o ângulo entre os vetores e influenciam no volume final.

## Exercicio 2

Dados: 
  
$$ X \sim N_3(\mathbf{\mu}, \Sigma) \\ 
\Sigma = \begin{bmatrix}
1 & -2 & 0 \\ 
-2 & 5 & 0 \\
0 & 0 & 2
\end{bmatrix} \\ \mathbf{\mu}^\intercal = [-3, 1, 4] $$ 

### a) Escreva genericamente a forma quadrática $d^2 = (\mathbf{x} - x\mathbf{\mu})^\intercal\Sigma^{-1}(\mathbf{x} - \mathbf{\mu})$ decomposta em função de uma soma envolvendo autovetores e autovalores de $\Sigma$.

$d^2$ pode ser escrito como: 
$$ d^2 = \sum_{i=1}^{p}{\frac{1}{\lambda_i}[(\mathbf{x}-\mathbf{\mu})^\intercal\mathbf{e_i}]^2}$$
onde $[\lambda_i, \mathbf{e_i}]$ são os autopares de $\Sigma$ (ou seja, autovalores e autovetores da matriz de covariância $\Sigma$).

### b) Encontre o comprimento dos eixos do elipsoide formado por $d^2$, com 0.95 de probabilidade (Encontre os autovalores no R usando a função _eigen_).

Primeiro, vamos determinar os autovalores e autovetores: 

```{r}
sigma = cbind(c(1,-2,0), c(-2,5,0), c(0,0,2))
mu = as.matrix(c(-3, 1, 4))

eig = eigen(sigma)
eig
```

Para definir o contorno da elipsoide de 95% de confiança, vamos utilizar o fato de que $(x-\mathbf{\mu})^\intercal\Sigma^{-1}(x-\mathbf{\mu}) \sim \chi_p^2$. Queremos uma elipse com área que contemple 95% dos nossos dados que possuem distribuição Normal de três variáveis, portanto, 
$(x-\mathbf{\mu})^\intercal\Sigma^{-1}(x-\mathbf{\mu}) = c^2$ onde $c^2$ é o quantil da distribuição Qui-Quadrado tal que $P(\chi_3^2 \leq c^2) = 0.95$.

Encontrando $c^2$ e calculando o comprimento dos eixos partindo do centro ($\mathbf{\mu}$):
```{r}
c2 = qchisq(0.95, 3)
c2

eixo1 = sqrt(c2)*sqrt(eig$values[1])
eixo1
eixo2 = sqrt(c2)*sqrt(eig$values[2])
eixo2
eixo3 = sqrt(c2)*sqrt(eig$values[3])
eixo3
```


Assim, temos que o comprimento dos eixos a partir do centro ($\mathbf{\mu}$) se dão por: 
$$ eixo_1 = c\sqrt{\lambda_1}\mathbf{e_1} = \sqrt{`r c2`}\sqrt{`r eig$values[1]`}[`r eig$vectors[,1]`] = `r eixo1`$$
$$ eixo_2 = c\sqrt{\lambda_2}\mathbf{e_2} = \sqrt{`r c2`}\sqrt{`r eig$values[2]`}[`r eig$vectors[,2]`] = `r eixo2`$$
$$ eixo_3 = c\sqrt{\lambda_3}\mathbf{e_3} = \sqrt{`r c2`}\sqrt{`r eig$values[3]`}[`r eig$vectors[,3]`] = `r eixo3`$$

A orientação se dá pelos vetores: $$e_1^\intercal = [`r eig$vectors[,1]`] $$ $$ e_2^\intercal = [`r eig$vectors[,2]`]$$ $$ e_3^\intercal = [`r eig$vectors[,3]`]$$

### c) Encontre a distribuição do vetor aleatório $\mathbf{Y}^\intercal = (Y_1, Y_2)$, onde $Y_1 = \frac{(X_1+X_2)}{2}$ e $Y_2 = 2X_1-X_2+X_3$

Sabemos que, se $X\sim N(\mathbf{\mu}, \Sigma)$,  combinações lineares dos componentes de X possuem distribuição normal e que os subsets de componentes de X também são normalmente distribuídos. 
Podemos reescrever $\mathbf{Y}$ como uma combinação $AX$:

$$\mathbf{Y} = \begin{bmatrix}
                Y_1 \\ Y_2 
                \end{bmatrix} = \begin{bmatrix}
                                \frac{(X_1+X_2)}{2} \\ 
                                2X_1-X_2+X_3
                                \end{bmatrix}$$
                                
$$ A = \begin{bmatrix}
        \frac{1}{2} & \frac{1}{2} & 0 \\
        2 & -1 & 1 
        \end{bmatrix}$$

$$ AX = \begin{bmatrix}
        \frac{1}{2}X_1 + \frac{1}{2}X_2 \\
        2X_1 - 1X_2 + 1X_3 
        \end{bmatrix}$$

Utilizando o *Resultado 4.3* do livro, temos que $AX \sim N_2(A\mathbf{\mu}, A\Sigma A^\intercal)$, onde:
$$ A\mathbf{\mu} = \begin{bmatrix}
                    \frac{1}{2} & \frac{1}{2} & 0 \\
                    2 & -1 & 1 
                    \end{bmatrix} \begin{bmatrix} -3 \\ 1 \\ 4 \end{bmatrix} = \begin{bmatrix} -1 \\ -3 \end{bmatrix}$$

$$ A \Sigma A^\intercal = \begin{bmatrix}
                    \frac{1}{2} & \frac{1}{2} & 0 \\
                    2 & -1 & 1
                    \end{bmatrix} \begin{bmatrix}
                                    1 & -2 & 0 \\ 
                                    -2 & 5 & 0 \\
                                    0 & 0 & 2
                                    \end{bmatrix} \begin{bmatrix} 
                                                  \frac{1}{2} & 2 \\
                                                  \frac{1}{2} & -1 \\
                                                            0 & 1 
                                                  \end{bmatrix} = 
\begin{bmatrix}
\frac{1}{2} & - \frac{5}{2} \\
- \frac{5}{2} & 19 \end{bmatrix} $$
                    
```{r}
A = rbind(c(1/2, 1/2, 0), c(2, -1, 1))
sigma = cbind(c(1,-2, 0), c(-2, 5, 0), c(0,0,2))
t(A)
A%*%sigma%*%t(A)
mu = as.matrix(c(-3,1,4))
A%*%mu
```


## Exercício 3

Dados: 

```{r, include = F, echo = F}
require(GGally)
require(mvShapiroTest)
require(carData)
```


```{r}
#x1: education (Gasto per capita com educação)
#x2: income (Renda per capita)
#x3: young (Proporção da pop abaixo de 18 anos)
#x4: urban (Proporção d pop na área urbana)
x=carData::Anscombe
head(x)
```


### Aplique o teste de Shapiro de normalidade univariada e bivariada. Informe o valor p de cada teste e conclua a respeito.

O teste Shapiro de normalidade se baseia na estatística $W$ e testa a hipótese nula de que os dados da amostra provém de uma distribuição normal. 
Dada um amostra $y_1, ..., y_n$, a estatística $W$ é dada por: 
$$ W = \frac{b^2}{s^2} $$
onde, 
$$ s^2 = \sum_{i=1}^{n}(y_i - \bar{y})^2 $$
Se $n$ é par, $n = 2k$ e $b$ é calculado por:
$$ b = \sum_{i=1}^{k}a_{n-i+1}(y{n-i+1} - y_i)$$
Se $n$ é ímpar, $n=2k+1$ e $b$ se calcula da seguinte forma:
$$ b = a_n(y_n-y_1) + ... + a_{k+2}(y_{k+2} - y_k) $$
Os valores de $a$ são tabelados e dependem de $n$. 

Para executar o teste, podemos utilizar uma função pronta do R e analisar o p-valor resultante. 

No caso do teste univariado, vamos executar o teste Shapiro para cada variável separadamente e avaliar a sua normalidade através dos p-valores encontrados:

```{r}
W=rep(0,ncol(x))

for(i in 1:4){
W[i]=shapiro.test(x[,i])$p.value }
W = t(as.matrix(W))
colnames(W) = c("x1", "x2", "x3", "x4")
rownames(W) = c("p-value")
W
```

Observa-se pelos resultados que não rejeitamos a hipótese nula de que as variáveis $x_2$ e $x_4$ provém de uma distribuição normal, com nível de significância $\alpha = 0.01$ $\alpha = 0.05$ e $\alpha = 0.10$. Já para a variável $x1$, rejeitamos para $\alpha = 0.05$ e $\alpha = 0.10$ e não rejeitamos para $\alpha = 0.01$. Por fim, não há evidência de que os dados coletados da variável $x_3$ venham de uma distribuição normal.



