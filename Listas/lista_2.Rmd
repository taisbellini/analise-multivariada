---
title: "Lista 2 - EST0204"
author: "Tais Bellini"
date: "12/18/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, tidy.opts=list(width.cutoff=60), tidy = T)
```

## Cap 8

### 8.1 Determinar os componentes principais $Y_1$ e $Y_2$ da população e a proporção da população explicada pelo primeiro componente. 

Dados: 

$$ \Sigma = \begin{bmatrix}
            5 & 2 \\
            2 & 2 
            \end{bmatrix} $$
            
Temos que:
$Y_i = \mathbf{e_i}^\intercal X$ onde, $\mathbf{e_i}$ é o autovetor associado ao i-ésimo autovalor $\lambda_i$.

Calculando os autovetores:
```{r}
S = matrix(c(5,2,2,2), ncol = 2)
eig = eigen(S)
eig
```

Assim,

$$Y_1 = \begin{bmatrix}
        -0.89 \\ -0.44
        \end{bmatrix} \mathbf{X}$$

$$Y_2 = \begin{bmatrix}
        0.44 \\ -0.89
        \end{bmatrix} \mathbf{X}$$

A proporção da variância populacional explicada (PVE) pelo componente $Y_1$ se dá da seguinte forma: 
$$ Proportion = \frac{\lambda_1}{\lambda_1+\lambda_2} = \frac{6}{6+1}$$

Portanto, temos que a $PVE_1$ = `r eig$values[1]/(eig$values[1]+eig$values[2])`.

### 8.2 

#### a) Determinar os componentes principais $Y_1$ e $Y_2$ da população a partir de $\rho$ e a proporção da população explicada pelo primeiro componente. 

Calculando a matriz de correlação $\rho$ (de duas formas diferentes):
```{r}
S = matrix(c(5,2,2,2), ncol = 2)
rho = solve(diag(sqrt(diag(S)))) %*% S %*% solve(diag(sqrt(diag(S))))
rho2 = cov2cor(S)
```

Determinando $Y_1$ e $Y_2$: 

```{r}
eig_rho = eigen(rho)
eig_rho
```

$$Y_1 = \begin{bmatrix}
        -0.707 \\ -0.707
        \end{bmatrix} \mathbf{X}$$

$$Y_2 = \begin{bmatrix}
        0.707 \\ -0.707
        \end{bmatrix} \mathbf{X}$$

A proporção da variância populacional explicada (padornizada) (PVE_std) pelo componente $Y_1$ se dá da seguinte forma: 
$$ Proportion = \frac{\lambda_1}{p} = \frac{1.63}{2}$$

Portanto, temos que a $PVEStd_1$ = `r eig_rho$values[1]/ncol(S)`.

#### b) Compare os componentes encontrados. São os mesmos? Deveriam ser?

Não são os mesmos. Observamos que, quando utilizamos $\Sigma$, a influência das variáveis $X_1$ e $X_2$ nos componentes diferem, já quando usamos $\rho$, as variáveis contribuem na mesma proporção. A proporção da variância explicada por $Y_1$ difere um pouco, mas em ambas fica entre 0.8 e 0.86.

#### c) Compute a correlação $\rho_{Y_1,Z_1}$, $\rho_{Y_1,Z_2}$ e $\rho_{Y_2,Z_1}$.

Sabemos que $\rho_{Y_i,Z_k} = e_{ik}\sqrt{\lambda_i}$. Portanto,

$$ \rho_{Y_1,Z_1} = e_{11}\sqrt{\lambda_1} = -0.707\sqrt{1.63} = -0.9$$
$$ \rho_{Y_1,Z_2} = e_{12}\sqrt{\lambda_1} = 0.707\sqrt{1.63} = 0.9$$
$$ \rho_{Y_2,Z_1} = e_{21}\sqrt{\lambda_1} = -0.707\sqrt{1.63} = -0.42$$

### 8.3 Determinar os componentes principais $Y_1$, $Y_2$ e $Y_3$. O que se pode afirmar sobre os autovetores (e componentes principais) associados com os autovalores que não são distintos?
Dados:

$$\Sigma = \begin{bmatrix}
            2 & 0 & 0 \\
            0 & 4 & 0 \\
            0 & 0 & 4 
            \end{bmatrix}$$
            
```{r}
S = matrix(c(2,0,0,0,4,0,0,0,4), ncol = 3)
eigen(S)
```
         
$$Y_1 = \begin{bmatrix}
        0 \\ 0 \\ 1
        \end{bmatrix} \mathbf{X} = X_3$$

$$Y_2 = \begin{bmatrix}
        0 \\ 1 \\ 0
        \end{bmatrix} \mathbf{X} = X_2$$

$$Y_3 = \begin{bmatrix}
        0 \\ 0 \\ 1
        \end{bmatrix} \mathbf{X} = X_1$$

Percebe-se que não há ganho em utilizar PCA neste caso, pois os componetnes principais são as proprias variáveis. Os dois primeiros componentes explicam a mesma proporção da variância, por possuir o mesmo autovalor.

### 8.4 Encontrar os componentes principais e PVE.

Dados:
$$\Sigma = \begin{bmatrix}
          \sigma^2 & \sigma^2\rho & 0 \\
          \sigma^2\rho & \sigma^2 & \sigma^2\rho \\
          0 & \sigma^2\rho & \sigma^2 
          \end{bmatrix}$$

Para determinar os autovalores, temos que: 

$$|\Sigma - I\lambda| = 0$$
$$(\sigma^2-\lambda)^3 - (\sigma^2-\lambda)(\sigma^2\rho)^2 - (\sigma^2-\lambda)(\sigma^2\rho)^2 = 0$$ 

$$(\sigma^2-\lambda)^3 - 2(\sigma^2-\lambda)(\sigma^2\rho)^2 = 0$$
$$(\sigma^2-\lambda)[(\sigma^2-\lambda) - 2(\sigma^2\rho)^2] = 0$$
Portanto, vamos ter o resultado zero na equação quando: $\sigma^2-\lambda = 0$, ou seja, $\lambda_1 = \sigma^2$; ou quando $(\sigma^2-\lambda)^2-2\sigma^4\rho^2 = 0$, ou seja, $\lambda_2 = \sigma^2(1+\sqrt{2}\rho)$ e $\lambda_3 = \sigma^2(1-\sqrt{2}\rho)$.


Encontrando os autovetores: 

Para $\lambda_1 = \sigma^2$:
$$ \left[\Sigma - I\lambda\right]e_1 = 0$$
$$ \begin{bmatrix}
          \sigma^2-\sigma^2 & \sigma^2\rho & 0 \\
          \sigma^2\rho & \sigma^2-\sigma^2 & \sigma^2\rho \\
          0 & \sigma^2\rho & \sigma^2-\sigma^2 
          \end{bmatrix} \begin{bmatrix} e_{11} \\ e_{21} \\ e_{31}\end{bmatrix} = 0$$
$$ \begin{bmatrix}
          0 & \sigma^2\rho & 0 \\
          \sigma^2\rho & 0 & \sigma^2\rho \\
          0 & \sigma^2\rho & 0 
          \end{bmatrix} \begin{bmatrix} e_{11} \\ e_{21} \\ e_{31}\end{bmatrix} = 0$$

          
$$ e_1 = \begin{bmatrix} -1 \\ 0 \\ 1 \end{bmatrix}$$ 

Normalizando: 

$$ e_1 = \begin{bmatrix} \frac{-1}{\sqrt{2}} \\ 0 \\ \frac{1}{\sqrt{2}} \end{bmatrix}$$ 
Para $\lambda_2 = \sigma^2(1+\sqrt{2}\rho)$:
$$ \left[\Sigma - I\lambda\right]e_2 = 0$$
$$ \begin{bmatrix}
          \sigma^2-\sigma^2(1+\sqrt{2}\rho) & \sigma^2\rho & 0 \\
          \sigma^2\rho & \sigma^2-\sigma^2(1+\sqrt{2}\rho) & \sigma^2\rho \\
          0 & \sigma^2\rho & \sigma^2-\sigma^2(1+\sqrt{2}\rho) 
          \end{bmatrix} \begin{bmatrix} e_{11} \\ e_{21} \\ e_{31}\end{bmatrix} = 0$$
$$ \begin{bmatrix}
          -\sigma^2\sqrt{2}\rho & \sigma^2\rho & 0 \\
          \sigma^2\rho & -\sigma^2\sqrt{2}\rho & \sigma^2\rho \\
          0 & \sigma^2\rho & -\sigma^2\sqrt{2}\rho 
          \end{bmatrix} \begin{bmatrix} e_{11} \\ e_{21} \\ e_{31}\end{bmatrix} = 0$$

$$ e_2 = \begin{bmatrix} \frac{1}{\sqrt{2}} \\ 1 \\ \frac{1}{\sqrt{2}} \end{bmatrix}$$ 

Normalizando: 

$$ e_2 = \begin{bmatrix} \frac{1}{2} \\ \frac{1}{\sqrt{2}} \\ \frac{1}{2} \end{bmatrix}$$ 

Para $\lambda_3 = \sigma^2(1-\sqrt{2}\rho)$:
$$ \left[\Sigma - I\lambda\right]e_3 = 0$$
$$ \begin{bmatrix}
          \sigma^2-\sigma^2(1-\sqrt{2}\rho) & \sigma^2\rho & 0 \\
          \sigma^2\rho & \sigma^2-\sigma^2(1-\sqrt{2}\rho) & \sigma^2\rho \\
          0 & \sigma^2\rho & \sigma^2-\sigma^2(1-\sqrt{2}\rho) 
          \end{bmatrix} \begin{bmatrix} e_{11} \\ e_{21} \\ e_{31}\end{bmatrix} = 0$$
$$ \begin{bmatrix}
          \sigma^2\sqrt{2}\rho & \sigma^2\rho & 0 \\
          \sigma^2\rho & \sigma^2\sqrt{2}\rho & \sigma^2\rho \\
          0 & \sigma^2\rho & \sigma^2\sqrt{2}\rho 
          \end{bmatrix} \begin{bmatrix} e_{11} \\ e_{21} \\ e_{31}\end{bmatrix} = 0$$

$$ e_3 = \begin{bmatrix} -\frac{1}{\sqrt{2}} \\ 1 \\ -\frac{1}{\sqrt{2}} \end{bmatrix}$$ 

Normalizando: 

$$ e_3 = \begin{bmatrix} -\frac{1}{2} \\ \frac{1}{\sqrt{2}} \\ -\frac{1}{2} \end{bmatrix}$$ 

### 8.10

Dados: 
```{r}
data = read.delim("../Wichern_data/T8-4.DAT", header = F, sep = "", stringsAsFactors = F)
colnames(data) = c("JP Morgan", "CitiBank", "Wells Fargo", "Royal Dutch Shell", "Exxon Mobil")
head(data)
```

#### a) Covariancia amostral e componentes principais

```{r}
S = cov(data)
S

pca = prcomp(data, scale =T)
pca
```

#### b) 
```{r}
summary(pca)
```

```{r}
biplot(pca)
```

Observamos que proporção da variância explicada pelas 3 primeiras componentes principais é de 0.89. Na primeira componente principal, temos uma soma ponderada de todos os retornos das ações das diferentes empresas, indicando ser um componente de mercado. Já a segunda faz uma separação entre JP Morgan, Citibank e Wells Fargo dos demais, sugerindo uma componente voltada ao tipo de indústria (banco ou combustível). A terceira componente não possui interpretação clara. 

#### c) Construir os intervalos de confiança simultâneos de Bonferroni com 90% de confiança para as variâncias $\lambda_1, \lambda_2$ e $\lambda_3$.

Como temos um n = 103, podemos utilizar os seguintes resultados para calcular os intervalos de Bonferroni: 
$$ \frac{\hat{\lambda_i}}{(1+z(\alpha/2)\sqrt{2/n})} \leq \lambda_i \leq \frac{\hat{\lambda_i}}{(1-z(\alpha/2)\sqrt{2/n})}$$

Portanto, temos que:
```{r}
n = nrow(data)
alpha = 0.1
z = qnorm(1-(alpha/2))
lambdas = eigen(S)$values

intervalos = matrix(nrow = 2, ncol = 3)
colnames(intervalos) = c("lambda1", "lambda2", "lambda3")
rownames(intervalos) = c("Linf", "Lsup")

for (i in 1:3){
  intervalos[1, i] = lambdas[i]/(1+z*sqrt(2/n))
  intervalos[2, i] = lambdas[i]/(1-z*sqrt(2/n))
}
intervalos
```

#### d) Os dados da taxa de retorno das ações podem ser explicados com menos variaveis?

Sim, observamos no exercicio b que quase 90% da variância é explicada pelas primeiras 3 componentes. O gráfico abaixo apresenta uma visão gráfica deste resultado.

```{r}
screeplot(pca, type = "l")
```

### 8.11 

Dados: 
```{r}
remove(list = ls())
data = read.delim("../Wichern_data/T8-5.DAT", header = F, sep = "", stringsAsFactors = F)
data[,5] = sapply(data[,5], function(x){x*10})
colnames(data) = c("Total pop(thousands)", "Professional Degree(%)", "Employed over 16(%)", "Govmt employment(%)", "Median home value($10.000)")
head(data)
```

#### a) Obter a matriz de covariancia

```{r}
S = cov(data)
S
```

#### b) Obter os autovalores e autovetores das 2 primeiras componentes principais.

```{r}
eig = eigen(S)
eig
```

#### c) Computar a proporção da variância explicada pelas primeiras duas componentes principais e calcular os coeficientes de correlação. Comparar com os resultados do exemplo 8.3.

```{r}
pca = prcomp(data, scale = F)
pca
summary(pca)

#calculando pelas formulas
prop1 = eig$values[1]/sum(eig$values)
prop2 = eig$values[2]/sum(eig$values)
prop1+prop2
```

Aqui, temos uma proporção acumulada de 0.79 nas duas primeiras componentes principais.

```{r}
t(cor(pca$x, data))
```

Observa-se que a primeira componente principal está mais correlacionada com emprego, contrastando o percentual total de empregados vs. o percentual de empregados pelo governo. Isto não mudou em relação ao exercício 8.3. Já a componente principal 2, observamos o peso de diploma, emprego e valor mediano de imóvel. Observamos que a variável cuja escala foi alterada (Median home value) passa a fazer parte significativa na componente principal 2, em comparação com o exercício 8.3.

### 8.18
Dados: 

```{r}
remove(list = ls())
data = read.delim("../Wichern_data/T1-9.DAT", header = F, sep = "\t", stringsAsFactors = F)
colnames(data) = c("Country", "100m", "200m", "400m", "800m", "1500m", "3000m", "Marathon")
head(data)
```

#### a) Obtenha a matriz de correlação e determine os autovalores e autovetores.

```{r}
R = cor(data[,-1])
eig = eigen(R)
eig
```

#### b) Determine as duas primeiras componentes principais para as variaveis padronizadas. Prepare uma tabela com a correlação entre as variáveis padronizadas com os componentes e o percentual cumulativo da variância total (padronizada) explicada pelos dois componentes.

```{r}
pca = prcomp(data[,-1], scale. = T)
pca$rotation[,1:2]
pca_summary = summary(pca)
paste("Cumulative proportion: ", pca_summary$importance[3,2])
print("Correlation between variables and PC")
t(cor(pca$x, data[,-1])[1:2,])
```

#### c) Interprete as componentes principais

A componente 1 é uma soma ponderada da velocidade de cada nação para cada distância. A segunda componente aparenta fazer um contraste entre distâncias curtas e longas, sendo 800m a primeira distância mais longa.

#### d) Ordene pela primeira CP e analise

```{r}
n = nrow(data)
ss=data.frame(ord=seq(1,n,by=1), y=pca$x[,1])
ss_ord = ss[order(ss[,2], decreasing=T),]
data[ss_ord[1:3,1],1]
```

Sim, EUA, Alemanha e Russia fazem sentido como sendo os países com melhor desempenho.


## Cap 9

### 9.1 Escreva a matriz de covariancia $\rho$ no formato $\rho = LL^\intercal + \Psi$.

Dados:
$$\rho = \begin{bmatrix}
          1 & 0.63 & 0.45 \\
          0.63 & 1 & 0.35 \\
          0.45 & 0.35 & 1
          \end{bmatrix}$$
        
$$ Z_1 = 0.9F_1 + \epsilon_1$$
$$ Z_2 = 0.7F_1 + \epsilon_2$$
$$ Z_3 = 0.5F_1 + \epsilon_3$$
$$Var(F_1) = 1$$
$$Cov(\epsilon, F_1) = 0$$
$$\Psi = Cov(\epsilon) = \begin{bmatrix}
                        0.19 & 0 & 0 \\
                        0 & 0.51 & 0 \\
                        0 & 0 & 0.75 
                        \end{bmatrix}$$
                        

Temos que $L^\intercal = [0.9, 0.7, 0.5]$. Portanto, 
$$LL^\intercal = \begin{bmatrix} 0.9 \\ 0.7 \\ 0.5 \end{bmatrix} [0.9, 0.7, 0.5] = \begin{bmatrix}
0.81 & 0.63 & 0.45 \\
0.63 & 0.49 & 0.35 \\
0.45 & 0.35 & 0.25
\end{bmatrix}$$.

Logo, podemos verificar que 
$$\rho = \begin{bmatrix}
          1 & 0.63 & 0.45 \\
          0.63 & 1 & 0.35 \\
          0.45 & 0.35 & 1
          \end{bmatrix} = LL^\intercal + \Psi = \begin{bmatrix} 0.9 \\ 0.7 \\ 0.5 \end{bmatrix} [0.9, 0.7, 0.5] + \begin{bmatrix}
                        0.19 & 0 & 0 \\
                        0 & 0.51 & 0 \\
                        0 & 0 & 0.75 
                        \end{bmatrix}$$

Cálculos no R:
```{r}
L = matrix(c(0.9,0.7,0.5))
psi = matrix(c(0.19, 0, 0, 0, 0.51, 0, 0, 0, 0.75), ncol = 3)
L%*%t(L) + psi
```

### 9.2 

#### a) Calcular $h_i$, i = 1, 2, 3 e interpretar.

Para número de fatores m = 1:

$$h_1^2 = 0.9^2 = 0.81$$
$$h_2^2 = 0.7^2 = 0.49$$
$$h_3^2 = 0.5^2 = 0.25$$
As comunalidades são as partes da variância da variável que são explicadas por aquele fator.

#### b) Cor($Z_i, F_1$). Qual variável pode ter o maior peso em nomear o fator comum? Por que?

Sabemos que: 

$Cor(X_i, F_j) = Cov(X_i, F_i) = l_{ii}$

Logo, 

$$Cor(Z_1, F_1) = l_{11} = 0.9 $$

$$Cor(Z_2, F_1) = l_{21} = 0.7 $$

$$Cor(Z_3, F_1) = l_{31} = 0.5 $$

A variável $Z_1$ é a que tem maior correlação com o fator, portanto, provavelmente carregará o maior peso para nomear o fator.

### 9.3

Dados: 

$$ \lambda_1 = 1.96,\ e_1^\intercal = [0.625, 0.593, 0.507]$$
$$ \lambda_2 = 0.68,\ e_2^\intercal = [-0.219, -0.491, 0.843]$$
$$ \lambda_3 = 0.36,\ e_3^\intercal = [0.749, -0.638, -0.177]$$

#### a) Obter a matriz L e a matriz $\Psi$ usando a solução de PCA (fatores m = 1). Comparar com os resultados de 9.1.

Sabemos que, pelo método das componentes principais, temos:

$$ L = [\sqrt{\lambda_1}e_1, \sqrt{\lambda_2}e_2, ..., \sqrt{\lambda_p}e_p]$$

Portanto, 

$$ L = \begin{bmatrix}
        \sqrt{1.96}0.625 & \sqrt{0.68}(-0.219) & \sqrt{0.36}(0.749) \\
        \sqrt{1.96}0.593 & \sqrt{0.68}(-0.491) & \sqrt{0.36}(-0.638) \\
        \sqrt{1.96}0.507 & \sqrt{0.68}(0.843) & \sqrt{0.36}(-0.177) \\
        \end{bmatrix}$$
        
Considerando m = 1: 

$$ L = \begin{bmatrix}
        \sqrt{1.96}0.625 \\
        \sqrt{1.96}0.593 \\
        \sqrt{1.96}0.507  \\
        \end{bmatrix}$$
        
```{r}
lambda = 1.96
e = matrix(c(0.625, 0.593, 0.507), ncol = 1)

sqrt(lambda[1])*e
```
Diferente do exrcicio 9.1.

#### b) Qual a proporção da variância da população total é explicada pelo primeiro fator comum?

Como estamos lidando com a matrix de correlacão $\rho$, podemos calcular a variância explicada pelo primeiro fator da seguinte forma: $\frac{\lambda_1}{p} = \frac{1.96}{3} = 0.65$.

### 9.10 
Dados:

```{r}
remove(list = ls())
data = read.table("../Wichern_data/E9-14.dat", fill=T, header=F,sep="")
data[,6] = c(0,0,0,0,0,1.000,0)
data = data[-7,]
data[upper.tri(data)] = t(data)[upper.tri(data)]
colnames(data) = rownames(data) = c("Skull length", "Skull breadth", "Femur Length", "Tibia Length", "Humerus length", "Ulna Length")
data
```

Utilizando os escores dos fatores estimados não rotacionados, obter:
#### a) Variância específica

```{r}
fac = factanal(covmat = as.matrix(data), factors = 2, rotation = "none")
v = fac$uniquenesses
v
```

#### b) Comunalidades

```{r}
comm = apply(fac$loadings, 1, function(row){sum(row^2)})
comm
```

#### c) Proporção explicada por cada fator

```{r}
load = fac$loadings
load
```

Fator 1: 0.712
Fator 2: 0.024

#### d) Matriz residual $S_n - LL^\intercal - \hat{\Psi}$

```{r}
E=load%*%t(load)+diag(v)
Res=data-E
Res
```

### 9.12

Dados: 

$$ S = 10^{-3} \begin{bmatrix}
                    11.072 &   &  \\
                    8.019 & 6.417 & \\
                    8.160 & 6.005 & 6.773 
                    \end{bmatrix}$$
                    


\begin{table}[h!]
\begin{tabular}{|c|c|}
\hline
Variable & Estimated factor loadings (F1) \\ \hline
ln(length) & 0.1022 \\
ln(width)  & 0.0752 \\
ln(height) & 0.0765 \\ \hline
\end{tabular}
\end{table}

#### a) Variância específica

```{r}
S = matrix(c(11.072, 8.019, 8.160, 8.019, 6.417, 6.005, 8.160, 6.005, 6.773), ncol = 3)*10^-3
L = matrix(c(0.1022, 0.0752, 0.0765))
LL = L%*%t(L)
```


Sabemos que $Cov(X) = S = LL^\intercal + \psi$. Como temos os valores de $S$ e $L$, podemos encontrar a variância específica $\psi$: 

$$\psi_i = Sn_{ii} - \hat{h_{i}^2}$$ onde:
$\hat{h_{i}^2} = l_{i1}^2$ pois temos apenas 1 fator (m=1) e $Sn = \frac{(n-1)}{n}S$, pois estamos utilizando o método da máxima verossimilhança. 

```{r}
n = 24
Sn = (n-1)/n*S
psi = diag(Sn) - diag(LL)
psi
```

#### b) Comunalidades

Calculado acima: $h_i^2 = l_{i1}^2$. 
```{r}
diag(LL)
```

#### c) Proporção explicada pelo fator

```{r}
sum(diag(LL))/sum(diag(Sn))
```

#### d) Matriz residual

```{r}
Sn - LL - psi
```

## Cap 12

### 12.1

Dados: 

```{r}
remove(list = ls())
data = data.frame(
  President = c("Reagan", "J. Carter", "G. Ford", "R. Nixon", "L. Johnson", "J. Kennedy"),
  BirthPlace = c("Midwest", "South", "Midwest", "West", "South", "East"),
  ElectedFirstTerm = c("Yes", "Yes", "No", "Yes", "No", "Yes"),
  Party = c("Rep", "Dem", "Rep", "Rep", "Dem", "Dem"),
  PriorEx = c("No", "No",  "Yes", "Yes", "Yes", "Yes"),
  VicePres = c("No", "No", "Yes", "Yes", "Yes", "No"), stringsAsFactors = T
)
data
```

#### a) 

Reagan e Carter: 
\begin{table}[h!]
\begin{tabular}{cc|cc}
       &   & Reagan &   \\
       &   & 1      & 0 \\ \hline
Carter & 1 & 1      & 1 \\
       & 0 & 1      & 2
\end{tabular}
\end{table}

