---
title: "Lista 2 - EST0204"
author: "Tais Bellini"
date: "12/18/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, tidy.opts=list(width.cutoff=60), tidy = T)
```

## Cap 8

### 8.1 Determinar os componentes principais $Y_1$ e $Y_2$ da população e a proporção da população explicada pelo primeiro componente. 

Dados: 

$$ \Sigma = \begin{bmatrix}
            5 & 2 \\
            2 & 2 
            \end{bmatrix} $$
            
Temos que:
$Y_i = \mathbf{e_i}^\intercal X$ onde, $\mathbf{e_i}$ é o autovetor associado ao i-ésimo autovalor $\lambda_i$.

Calculando os autovetores:
```{r}
S = matrix(c(5,2,2,2), ncol = 2)
eig = eigen(S)
eig
```

Assim,

$$Y_1 = \begin{bmatrix}
        -0.89 \\ -0.44
        \end{bmatrix} \mathbf{X}$$

$$Y_2 = \begin{bmatrix}
        0.44 \\ -0.89
        \end{bmatrix} \mathbf{X}$$

A proporção da variância populacional explicada (PVE) pelo componente $Y_1$ se dá da seguinte forma: 
$$ Proportion = \frac{\lambda_1}{\lambda_1+\lambda_2} = \frac{6}{6+1}$$

Portanto, temos que a $PVE_1$ = `r eig$values[1]/(eig$values[1]+eig$values[2])`.

### 8.2 

#### a) Determinar os componentes principais $Y_1$ e $Y_2$ da população a partir de $\rho$ e a proporção da população explicada pelo primeiro componente. 

Calculando a matriz de correlação $\rho$ (de duas formas diferentes):
```{r}
S = matrix(c(5,2,2,2), ncol = 2)
rho = solve(diag(sqrt(diag(S)))) %*% S %*% solve(diag(sqrt(diag(S))))
rho2 = cov2cor(S)
```

Determinando $Y_1$ e $Y_2$: 

```{r}
eig_rho = eigen(rho)
eig_rho
```

$$Y_1 = \begin{bmatrix}
        -0.707 \\ -0.707
        \end{bmatrix} \mathbf{X}$$

$$Y_2 = \begin{bmatrix}
        0.707 \\ -0.707
        \end{bmatrix} \mathbf{X}$$

A proporção da variância populacional explicada (padornizada) (PVE_std) pelo componente $Y_1$ se dá da seguinte forma: 
$$ Proportion = \frac{\lambda_1}{p} = \frac{1.63}{2}$$

Portanto, temos que a $PVEStd_1$ = `r eig_rho$values[1]/ncol(S)`.

#### b) Compare os componentes encontrados. São os mesmos? Deveriam ser?

Não são os mesmos. Observamos que, quando utilizamos $\Sigma$, a influência das variáveis $X_1$ e $X_2$ nos componentes diferem, já quando usamos $\rho$, as variáveis contribuem na mesma proporção. A proporção da variância explicada por $Y_1$ difere um pouco, mas em ambas fica entre 0.8 e 0.86.

#### c) Compute a correlação $\rho_{Y_1,Z_1}$, $\rho_{Y_1,Z_2}$ e $\rho_{Y_2,Z_1}$.

Sabemos que $\rho_{Y_i,Z_k} = e_{ik}\sqrt{\lambda_i}$. Portanto,

$$ \rho_{Y_1,Z_1} = e_{11}\sqrt{\lambda_1} = -0.707\sqrt{1.63} = -0.9$$
$$ \rho_{Y_1,Z_2} = e_{12}\sqrt{\lambda_1} = 0.707\sqrt{1.63} = 0.9$$
$$ \rho_{Y_2,Z_1} = e_{21}\sqrt{\lambda_1} = -0.707\sqrt{1.63} = -0.42$$

### 8.3 Determinar os componentes principais $Y_1$, $Y_2$ e $Y_3$. O que se pode afirmar sobre os autovetores (e componentes principais) associados com os autovalores que não são distintos?
Dados:

$$\Sigma = \begin{bmatrix}
            2 & 0 & 0 \\
            0 & 4 & 0 \\
            0 & 0 & 4 
            \end{bmatrix}$$
            
```{r}
S = matrix(c(2,0,0,0,4,0,0,0,4), ncol = 3)
eigen(S)
```
         
$$Y_1 = \begin{bmatrix}
        0 \\ 0 \\ 1
        \end{bmatrix} \mathbf{X} = X_3$$

$$Y_2 = \begin{bmatrix}
        0 \\ 1 \\ 0
        \end{bmatrix} \mathbf{X} = X_2$$

$$Y_3 = \begin{bmatrix}
        0 \\ 0 \\ 1
        \end{bmatrix} \mathbf{X} = X_1$$

Percebe-se que não há ganho em utilizar PCA neste caso, pois os componetnes principais são as proprias variáveis. Os dois primeiros componentes explicam a mesma proporção da variância, por possuir o mesmo autovalor.

### 8.4 Encontrar os componentes principais e PVE.

Dados:
$$\Sigma = \begin{bmatrix}
          \sigma^2 & \sigma^2\rho & 0 \\
          \sigma^2\rho & \sigma^2 & \sigma^2\rho \\
          0 & \sigma^2\rho & \sigma^2 
          \end{bmatrix}$$

Para determinar os autovalores, temos que: 

$$|\Sigma - I\lambda| = 0$$
$$(\sigma^2-\lambda)^3 - (\sigma^2-\lambda)(\sigma^2\rho)^2 - (\sigma^2-\lambda)(\sigma^2\rho)^2 = 0$$ 

$$(\sigma^2-\lambda)^3 - 2(\sigma^2-\lambda)(\sigma^2\rho)^2 = 0$$
$$(\sigma^2-\lambda)[(\sigma^2-\lambda) - 2(\sigma^2\rho)^2] = 0$$
Portanto, vamos ter o resultado zero na equação quando: $\sigma^2-\lambda = 0$, ou seja, $\lambda_1 = \sigma^2$; ou quando $(\sigma^2-\lambda)^2-2\sigma^4\rho^2 = 0$, ou seja, $\lambda_2 = \sigma^2(1+\sqrt{2}\rho)$ e $\lambda_3 = \sigma^2(1-\sqrt{2}\rho)$.


Encontrando os autovetores: 

Para $\lambda_1 = \sigma^2$:
$$ \left[\Sigma - I\lambda\right]e_1 = 0$$
$$ \begin{bmatrix}
          \sigma^2-\sigma^2 & \sigma^2\rho & 0 \\
          \sigma^2\rho & \sigma^2-\sigma^2 & \sigma^2\rho \\
          0 & \sigma^2\rho & \sigma^2-\sigma^2 
          \end{bmatrix} \begin{bmatrix} e_{11} \\ e_{21} \\ e_{31}\end{bmatrix} = 0$$
$$ \begin{bmatrix}
          0 & \sigma^2\rho & 0 \\
          \sigma^2\rho & 0 & \sigma^2\rho \\
          0 & \sigma^2\rho & 0 
          \end{bmatrix} \begin{bmatrix} e_{11} \\ e_{21} \\ e_{31}\end{bmatrix} = 0$$

          
$$ e_1 = \begin{bmatrix} -1 \\ 0 \\ 1 \end{bmatrix}$$ 

Normalizando: 

$$ e_1 = \begin{bmatrix} \frac{-1}{\sqrt{2}} \\ 0 \\ \frac{1}{\sqrt{2}} \end{bmatrix}$$ 
Para $\lambda_2 = \sigma^2(1+\sqrt{2}\rho)$:
$$ \left[\Sigma - I\lambda\right]e_2 = 0$$
$$ \begin{bmatrix}
          \sigma^2-\sigma^2(1+\sqrt{2}\rho) & \sigma^2\rho & 0 \\
          \sigma^2\rho & \sigma^2-\sigma^2(1+\sqrt{2}\rho) & \sigma^2\rho \\
          0 & \sigma^2\rho & \sigma^2-\sigma^2(1+\sqrt{2}\rho) 
          \end{bmatrix} \begin{bmatrix} e_{11} \\ e_{21} \\ e_{31}\end{bmatrix} = 0$$
$$ \begin{bmatrix}
          -\sigma^2\sqrt{2}\rho & \sigma^2\rho & 0 \\
          \sigma^2\rho & -\sigma^2\sqrt{2}\rho & \sigma^2\rho \\
          0 & \sigma^2\rho & -\sigma^2\sqrt{2}\rho 
          \end{bmatrix} \begin{bmatrix} e_{11} \\ e_{21} \\ e_{31}\end{bmatrix} = 0$$

$$ e_2 = \begin{bmatrix} \frac{1}{\sqrt{2}} \\ 1 \\ \frac{1}{\sqrt{2}} \end{bmatrix}$$ 

Normalizando: 

$$ e_2 = \begin{bmatrix} \frac{1}{2} \\ \frac{1}{\sqrt{2}} \\ \frac{1}{2} \end{bmatrix}$$ 

Para $\lambda_3 = \sigma^2(1-\sqrt{2}\rho)$:
$$ \left[\Sigma - I\lambda\right]e_3 = 0$$
$$ \begin{bmatrix}
          \sigma^2-\sigma^2(1-\sqrt{2}\rho) & \sigma^2\rho & 0 \\
          \sigma^2\rho & \sigma^2-\sigma^2(1-\sqrt{2}\rho) & \sigma^2\rho \\
          0 & \sigma^2\rho & \sigma^2-\sigma^2(1-\sqrt{2}\rho) 
          \end{bmatrix} \begin{bmatrix} e_{11} \\ e_{21} \\ e_{31}\end{bmatrix} = 0$$
$$ \begin{bmatrix}
          \sigma^2\sqrt{2}\rho & \sigma^2\rho & 0 \\
          \sigma^2\rho & \sigma^2\sqrt{2}\rho & \sigma^2\rho \\
          0 & \sigma^2\rho & \sigma^2\sqrt{2}\rho 
          \end{bmatrix} \begin{bmatrix} e_{11} \\ e_{21} \\ e_{31}\end{bmatrix} = 0$$

$$ e_3 = \begin{bmatrix} -\frac{1}{\sqrt{2}} \\ 1 \\ -\frac{1}{\sqrt{2}} \end{bmatrix}$$ 

Normalizando: 

$$ e_3 = \begin{bmatrix} -\frac{1}{2} \\ \frac{1}{\sqrt{2}} \\ -\frac{1}{2} \end{bmatrix}$$ 

### 8.10

Dados: 
```{r}
data = read.delim("../Wichern_data/T8-4.DAT", header = F, sep = "", stringsAsFactors = F)
colnames(data) = c("JP Morgan", "CitiBank", "Wells Fargo", "Royal Dutch Shell", "Exxon Mobil")
head(data)
```

#### a) Covariancia amostral e componentes principais

```{r}
S = cov(data)
S

pca = prcomp(data, scale =T)
pca
```

#### b) 
```{r}
summary(pca)
```

```{r}
biplot(pca)
```

Observamos que proporção da variância explicada pelas 3 primeiras componentes principais é de 0.89. Na primeira componente principal, temos uma soma ponderada de todos os retornos das ações das diferentes empresas, indicando ser um componente de mercado. Já a segunda faz uma separação entre JP Morgan, Citibank e Wells Fargo dos demais, sugerindo uma componente voltada ao tipo de indústria (banco ou combustível). A terceira componente não possui interpretação clara. 

#### c) Construir os intervalos de confiança simultâneos de Bonferroni com 90% de confiança para as variâncias $\lambda_1, \lambda_2$ e $\lambda_3$.

Como temos um n = 103, podemos utilizar os seguintes resultados para calcular os intervalos de Bonferroni: 
$$ \frac{\hat{\lambda_i}}{(1+z(\alpha/2)\sqrt{2/n})} \leq \lambda_i \leq \frac{\hat{\lambda_i}}{(1-z(\alpha/2)\sqrt{2/n})}$$

Portanto, temos que:
```{r}
n = nrow(data)
alpha = 0.1
z = qnorm(1-(alpha/2))
lambdas = eigen(S)$values

intervalos = matrix(nrow = 2, ncol = 3)
colnames(intervalos) = c("lambda1", "lambda2", "lambda3")
rownames(intervalos) = c("Linf", "Lsup")

for (i in 1:3){
  intervalos[1, i] = lambdas[i]/(1+z*sqrt(2/n))
  intervalos[2, i] = lambdas[i]/(1-z*sqrt(2/n))
}
intervalos
```

#### d) Os dados da taxa de retorno das ações podem ser explicados com menos variaveis?

Sim, observamos no exercicio b que quase 90% da variância é explicada pelas primeiras 3 componentes. O gráfico abaixo apresenta uma visão gráfica deste resultado.

```{r}
screeplot(pca, type = "l")
```

### 8.11 

Dados: 
```{r}
remove(list = ls())
data = read.delim("../Wichern_data/T8-5.DAT", header = F, sep = "", stringsAsFactors = F)
data[,5] = sapply(data[,5], function(x){x*10})
colnames(data) = c("Total pop(thousands)", "Professional Degree(%)", "Employed over 16(%)", "Govmt employment(%)", "Median home value($10.000)")
head(data)
```

#### a) Obter a matriz de covariancia

```{r}
S = cov(data)
S
```

#### b) Obter os autovalores e autovetores das 2 primeiras componentes principais.

```{r}
eig = eigen(S)
eig
```

#### c) Computar a proporção da variância explicada pelas primeiras duas componentes principais e calcular os coeficientes de correlação. Comparar com os resultados do exemplo 8.3.

```{r}
pca = prcomp(data, scale = F)
pca
summary(pca)

#calculando pelas formulas
prop1 = eig$values[1]/sum(eig$values)
prop2 = eig$values[2]/sum(eig$values)
prop1+prop2
```

Aqui, temos uma proporção acumulada de 0.79 nas duas primeiras componentes principais.

```{r}
t(cor(pca$x, data))
```

Observa-se que a primeira componente principal está mais correlacionada com emprego, contrastando o percentual total de empregados vs. o percentual de empregados pelo governo. Isto não mudou em relação ao exercício 8.3. Já a componente principal 2, observamos o peso de diploma, emprego e valor mediano de imóvel. Observamos que a variável cuja escala foi alterada (Median home value) passa a fazer parte significativa na componente principal 2, em comparação com o exercício 8.3.

### 8.18
Dados: 

```{r}
remove(list = ls())
data = read.delim("../Wichern_data/T1-9.DAT", header = F, sep = "\t", stringsAsFactors = F)
colnames(data) = c("Country", "100m", "200m", "400m", "800m", "1500m", "3000m", "Marathon")
head(data)
```

#### a) Obtenha a matriz de correlação e determine os autovalores e autovetores.

```{r}
R = cor(data[,-1])
eig = eigen(R)
eig
```

#### b) Determine as duas primeiras componentes principais para as variaveis padronizadas. Prepare uma tabela com a correlação entre as variáveis padronizadas com os componentes e o percentual cumulativo da variância total (padronizada) explicada pelos dois componentes.

```{r}
pca = prcomp(data[,-1], scale. = T)
pca$rotation[,1:2]
pca_summary = summary(pca)
paste("Cumulative proportion: ", pca_summary$importance[3,2])
print("Correlation between variables and PC")
t(cor(pca$x, data[,-1])[1:2,])
```

#### c) Interprete as componentes principais

A componente 1 é uma soma ponderada da velocidade de cada nação para cada distância. A segunda componente aparenta fazer um contraste entre distâncias curtas e longas, sendo 800m a primeira distância mais longa.

#### d) Ordene pela primeira CP e analise

```{r}
n = nrow(data)
ss=data.frame(ord=seq(1,n,by=1), y=pca$x[,1])
ss_ord = ss[order(ss[,2], decreasing=T),]
data[ss_ord[1:3,1],1]
```

Sim, EUA, Alemanha e Russia fazem sentido como sendo os países com melhor desempenho.


## Cap 9

### 9.1 Escreva a matriz de covariancia $\rho$ no formato $\rho = LL^\intercal + \Psi.

Dados:
$$\rho = \begin{bmatrix}
          1 & 0.63 & 0.45 \\
          0.63 & 1 & 0.35 \\
          0.45 & 0.35 & 1
          \end{bmatrix}$$
        
$$ Z_1 = 0.9F_1 + \epsilon_1$$
$$ Z_2 = 0.7F_1 + \epsilon_2$$
$$ Z_3 = 0.5F_1 + \epsilon_3$$
$$Var(F_1) = 1$$
$$Cov(\epsilon, F_1) = 0$$
$$\Psi = Cov(\epsilon) = \begin{bmatrix}
                        0.19 & 0 & 0 \\
                        0 & 0.51 & 0 \\
                        0 & 0 & 0.75 
                        \end{bmatrix}$$
                        

Temos que $L^\intercal = [0.9, 0.7, 0.5]$. Portanto, 
$$LL^\intercal = \begin{bmatrix} 0.9 \\ 0.7 \\ 0.5 \end{bmatrix} [0.9, 0.7, 0.5] = \begin{bmatrix}
0.81 & 0.63 & 0.45 \\
0.63 & 0.49 & 0.35 \\
0.45 & 0.35 & 0.25
\end{bmatrix}$$.

Logo, podemos verificar que 
$$\rho = \begin{bmatrix}
          1 & 0.63 & 0.45 \\
          0.63 & 1 & 0.35 \\
          0.45 & 0.35 & 1
          \end{bmatrix} = LL^\intercal + \Psi = \begin{bmatrix} 0.9 \\ 0.7 \\ 0.5 \end{bmatrix} [0.9, 0.7, 0.5] + \begin{bmatrix}
                        0.19 & 0 & 0 \\
                        0 & 0.51 & 0 \\
                        0 & 0 & 0.75 
                        \end{bmatrix}$$

Cálculos no R:
```{r}
L = matrix(c(0.9,0.7,0.5))
psi = matrix(c(0.19, 0, 0, 0, 0.51, 0, 0, 0, 0.75), ncol = 3)
L%*%t(L) + psi
```


